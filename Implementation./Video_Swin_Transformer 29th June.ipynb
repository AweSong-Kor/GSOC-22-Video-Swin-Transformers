{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Video Swin Transformer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IXEfrlCq474n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "nHL_EAhFnyAI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Dropout, Conv3D, LayerNormalization, GlobalAveragePooling1D"
      ],
      "metadata": {
        "id": "etqo8emfhLk5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGDhC4cZaP-7",
        "outputId": "45721b42-c185-48bc-8b96-3a1367f8eec4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[K     |████████████████████████████████| 431 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.12.0+cu113)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2.10)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from timm.models.layers import DropPath, trunc_normal_"
      ],
      "metadata": {
        "id": "O2Kw4ff2Z_E8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Declaring tensor with random values"
      ],
      "metadata": {
        "id": "kvf0EkdF8vOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_pt = torch.randint(255,(10,8,32,32,3))\n",
        "x_np = x_pt.numpy()\n",
        "x_tf = tf.convert_to_tensor(x_np)\n",
        "\n",
        "x_pt.shape , x_tf.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsH8RyYn5-aw",
        "outputId": "3b030fc5-0ede-4b59-8580-83fcebb8ce7b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 8, 32, 32, 3]), TensorShape([10, 8, 32, 32, 3]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Window Partition Function"
      ],
      "metadata": {
        "id": "uQFwBQUg817N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_window_size(x_size, window_size, shift_size=None):\n",
        "    use_window_size = list(window_size)\n",
        "    if shift_size is not None:\n",
        "        use_shift_size = list(shift_size)\n",
        "    for i in range(len(x_size)):\n",
        "        if x_size[i] <= window_size[i]:\n",
        "            use_window_size[i] = x_size[i]\n",
        "            if shift_size is not None:\n",
        "                use_shift_size[i] = 0\n",
        "\n",
        "    if shift_size is None:\n",
        "        return tuple(use_window_size)\n",
        "    else:\n",
        "        return tuple(use_window_size), tuple(use_shift_size)"
      ],
      "metadata": {
        "id": "b98XX11lQvpJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = (4,4,4)\n",
        "\n",
        "B, D, H, W, C = x_pt.shape\n",
        "window_size = get_window_size((D, H, W), window_size)\n",
        "window_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9uxwdB_Q2SC",
        "outputId": "d39c0843-0913-40a3-cbb1-c1e627c852ac"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 4, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch Window Partition"
      ],
      "metadata": {
        "id": "7cur8E5I6XuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import mul \n",
        "from functools import reduce\n"
      ],
      "metadata": {
        "id": "9ifB558C-Op9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def window_partition_pt(x, window_size):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: (B, D, H, W, C)\n",
        "        window_size (tuple[int]): window size\n",
        "    Returns:\n",
        "        windows: (B*num_windows, window_size*window_size, C)\n",
        "    \"\"\"\n",
        "    B, D, H, W, C = x.shape\n",
        "    x = x.view(B, D // window_size[0], window_size[0], H // window_size[1], window_size[1], W // window_size[2], window_size[2], C)\n",
        "    windows = x.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, reduce(mul, window_size), C)\n",
        "    return windows"
      ],
      "metadata": {
        "id": "UM1uaostAQzK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_windows_pt = window_partition_pt(x_pt, window_size)\n",
        "x_windows_pt.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4vuQ-Il6Wem",
        "outputId": "ec8e3454-9ce0-4a94-a095-ebd2f532eaed"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1280, 64, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF window partition func"
      ],
      "metadata": {
        "id": "971J0oT3QpN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import reduce"
      ],
      "metadata": {
        "id": "PZbEtBmR9Eas"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def window_partition_tf(x, window_size):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: (B, D, H, W, C)\n",
        "        window_size (tuple[int]): window size\n",
        "    Returns:\n",
        "        windows: (B*num_windows, window_size*window_size, C)\n",
        "    \"\"\"\n",
        "    B, D, H, W, C = x.shape\n",
        "    x = tf.reshape(x, [B, D // window_size[0], window_size[0], H // window_size[1], window_size[1], W // window_size[2], window_size[2], C])\n",
        "    windows = tf.reshape(tf.transpose(x, perm=[0, 1, 3, 5, 2, 4, 6, 7]), [-1, reduce((lambda x, y: x * y), window_size), C])                                    \n",
        "                                               \n",
        "    return windows"
      ],
      "metadata": {
        "id": "MkERjFtZ6cDn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_windows_tf = window_partition_tf(x_tf, window_size)\n",
        "x_windows_tf.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-ZwDIz4RB7v",
        "outputId": "28aa72c8-f6ce-4e48-d773-af67c4b8307f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1280, 64, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparision"
      ],
      "metadata": {
        "id": "MX8PWlpoBi-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.array_equal(x_windows_tf.numpy(), x_windows_pt.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eGnJhx2B3Hs",
        "outputId": "c21abf03-87b1-469e-d8a5-6b52d285f798"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Patch Merging"
      ],
      "metadata": {
        "id": "vTVe4zHzcmlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch"
      ],
      "metadata": {
        "id": "asamy8e8w79D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weight = []"
      ],
      "metadata": {
        "id": "o_TuvQlaor89"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2OORC5PpBok",
        "outputId": "7d085975-8971-4a9f-b1dc-c41fcd14972c"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchMerging_pt(nn.Module):\n",
        "    \"\"\" Patch Merging Layer\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
        "\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
        "        self.norm = norm_layer(4 * dim)\n",
        "\n",
        "        print(\"pytorch weights\\n\",self.reduction.weight.data.numpy())\n",
        "\n",
        "\n",
        "    def weight(self):\n",
        "        return  self.reduction.weight.data.numpy()\n",
        "    def forward(self, x):\n",
        "        \"\"\" Forward function.\n",
        "        Args:\n",
        "            x: Input feature, tensor size (B, D, H, W, C).\n",
        "        \"\"\"\n",
        "        B, D, H, W, C = x.shape\n",
        "\n",
        "        # padding\n",
        "        pad_input = (H % 2 == 1) or (W % 2 == 1)\n",
        "        if pad_input:\n",
        "            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n",
        "\n",
        "        x0 = x[:, :, 0::2, 0::2, :]  # B D H/2 W/2 C\n",
        "        x1 = x[:, :, 1::2, 0::2, :]  # B D H/2 W/2 C\n",
        "        x2 = x[:, :, 0::2, 1::2, :]  # B D H/2 W/2 C\n",
        "        x3 = x[:, :, 1::2, 1::2, :]  # B D H/2 W/2 C\n",
        "        x = torch.cat([x0, x1, x2, x3], -1)  # B D H/2 W/2 4*C\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.reduction(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "rFsHWhvZCAKK"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF"
      ],
      "metadata": {
        "id": "0cb4trrww_J4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, LayerNormalization, Normalization "
      ],
      "metadata": {
        "id": "6Z137rP1dkuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchMerging_tf(tf.keras.layers.Layer):\n",
        "    def __init__(self, dim,  weight, norm_layer=LayerNormalization):\n",
        "        super().__init__()\n",
        "        self.weight = weight\n",
        "        self.dim = dim\n",
        "        self.reduction = Dense(2 * dim, use_bias=False,  activation=None)\n",
        "        self.norm = norm_layer(epsilon=1e-5)\n",
        "        \n",
        "    def set_weight(self):\n",
        "        self.reduction.set_weights([np.transpose(self.weight)])\n",
        "        print(\"\\n\\ntf weights\\n\", np.transpose(self.reduction.get_weights()[0]))\n",
        "\n",
        "    # def build(self, input_shapes):\n",
        "    #     self.reduction.set_weights([np.transpose(self.weight)])\n",
        "\n",
        "    def call(self, x):\n",
        "        B, D, H, W, C = x.shape\n",
        "\n",
        "        # padding\n",
        "        pad_input = (H % 2 == 1) or (W % 2 == 1)\n",
        "        if pad_input:\n",
        "            x = tf.pad(x_tf, [[0,0], [0,0], [0,H%2], [0,W%2], [0,0]])\n",
        "\n",
        "        x0 = x[:, :, 0::2, 0::2, :]  # B D H/2 W/2 C\n",
        "        x1 = x[:, :, 1::2, 0::2, :]  # B D H/2 W/2 C\n",
        "        x2 = x[:, :, 0::2, 1::2, :]  # B D H/2 W/2 C\n",
        "        x3 = x[:, :, 1::2, 1::2, :]  # B D H/2 W/2 C\n",
        "\n",
        "        x = tf.concat([x0, x1, x2, x3], axis=-1) # B D H/2 W/2 4*C\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.reduction(x)\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "Kl5yYPvcdSSg"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_pt = torch.rand(10,8,32,32,3)\n",
        "x_np = x_pt.numpy()\n",
        "x_tf = tf.convert_to_tensor(x_np)"
      ],
      "metadata": {
        "id": "aiGVwoAuoQBo"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patchMerging_pt = PatchMerging_pt(3)\n",
        "\n",
        "patchMerging_tf = PatchMerging_tf(3, weight=patchMerging_pt.weight())\n",
        "\n",
        "# weight initialization \n",
        "x_tf_merge = patchMerging_tf(x_tf) \n",
        "patchMerging_tf.set_weight()\n",
        "\n",
        "\n",
        "x_pt_merge = patchMerging_pt(x_pt) \n",
        "x_tf_merge = patchMerging_tf(x_tf) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JM8MMYWQs149",
        "outputId": "6356c0dd-a648-41dd-bf8a-0f8f12d1f01b"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pytorch weights\n",
            " [[ 0.13929461 -0.23796813  0.18664165  0.17872916 -0.1861893   0.07879138\n",
            "   0.13968562  0.2091967  -0.22083881  0.2522571  -0.21668406 -0.1305449 ]\n",
            " [-0.10839953 -0.1357809  -0.01730531 -0.0049873   0.22164637  0.25438306\n",
            "  -0.14403766 -0.18755934 -0.12134344 -0.01897354  0.12711738 -0.23801711]\n",
            " [-0.18409483 -0.2080239   0.10796589  0.09955563  0.10302381 -0.00120286\n",
            "  -0.04078631 -0.23066    -0.10525169 -0.05158307  0.1598327  -0.0034953 ]\n",
            " [-0.09502082  0.07829095  0.05060059  0.23620865 -0.05604654  0.12544389\n",
            "  -0.25806972 -0.07644866  0.28568316 -0.26221994  0.14086194 -0.27208105]\n",
            " [ 0.08003189  0.05274725 -0.01824334  0.14429486  0.25617307 -0.24714327\n",
            "   0.25857413  0.22012089  0.16253045 -0.09206111 -0.27144745 -0.26858944]\n",
            " [ 0.21632424 -0.2575141  -0.2543203  -0.27531603 -0.04064408  0.0056396\n",
            "   0.22927107  0.17797421 -0.22900376  0.19352677 -0.19927354  0.2546929 ]]\n",
            "\n",
            "\n",
            "tf weights\n",
            " [[ 0.13929461 -0.23796813  0.18664165  0.17872916 -0.1861893   0.07879138\n",
            "   0.13968562  0.2091967  -0.22083881  0.2522571  -0.21668406 -0.1305449 ]\n",
            " [-0.10839953 -0.1357809  -0.01730531 -0.0049873   0.22164637  0.25438306\n",
            "  -0.14403766 -0.18755934 -0.12134344 -0.01897354  0.12711738 -0.23801711]\n",
            " [-0.18409483 -0.2080239   0.10796589  0.09955563  0.10302381 -0.00120286\n",
            "  -0.04078631 -0.23066    -0.10525169 -0.05158307  0.1598327  -0.0034953 ]\n",
            " [-0.09502082  0.07829095  0.05060059  0.23620865 -0.05604654  0.12544389\n",
            "  -0.25806972 -0.07644866  0.28568316 -0.26221994  0.14086194 -0.27208105]\n",
            " [ 0.08003189  0.05274725 -0.01824334  0.14429486  0.25617307 -0.24714327\n",
            "   0.25857413  0.22012089  0.16253045 -0.09206111 -0.27144745 -0.26858944]\n",
            " [ 0.21632424 -0.2575141  -0.2543203  -0.27531603 -0.04064408  0.0056396\n",
            "   0.22927107  0.17797421 -0.22900376  0.19352677 -0.19927354  0.2546929 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "round = 2\n",
        "np.array_equal( np.around(x_pt_merge.detach().numpy(), round) , np.around( x_tf_merge.numpy(), round))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvNeD-4a0l81",
        "outputId": "efa9b738-aeb0-4da9-a8b2-451f2662450a"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_pt_merge.shape, x_tf_merge.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PWfflZAth_a",
        "outputId": "ba818a5c-8c6e-408a-9b9d-1063f719e49e"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 8, 16, 16, 6]), TensorShape([10, 8, 16, 16, 6]))"
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print( x_pt_merge[0][0][0][1].detach().numpy(),\"\\n\\n\\n\", x_tf_merge[0][0][0][1].numpy() )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBcb_XPdxZjp",
        "outputId": "21bbee18-5aff-40f7-9673-c93ea4b6b992"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.47216618  0.2653861   0.25594187  0.45342708  0.00656515 -0.80686843] \n",
            "\n",
            "\n",
            " [-0.47216612  0.26538607  0.2559419   0.45342708  0.00656523 -0.80686843]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Patch Embed 3D"
      ],
      "metadata": {
        "id": "tPbV8U1WZIgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_pt = torch.rand(10,3,8,32,32)\n",
        "x_np = x_pt.numpy()\n",
        "x_tf = tf.convert_to_tensor(x_np)"
      ],
      "metadata": {
        "id": "3OlJ5kExfhYq"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbed3D_pt(nn.Module):\n",
        "    \"\"\" Video to Patch Embedding.\n",
        "    Args:\n",
        "        patch_size (int): Patch token size. Default: (2,4,4).\n",
        "        in_chans (int): Number of input video channels. Default: 3.\n",
        "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
        "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
        "    \"\"\"\n",
        "    def __init__(self, patch_size=(2,4,4), in_chans=3, embed_dim=96, norm_layer=None):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        self.in_chans = in_chans\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.proj = nn.Conv3d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        if norm_layer is not None:\n",
        "            self.norm = norm_layer(embed_dim)\n",
        "        else:\n",
        "            self.norm = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward function.\"\"\"\n",
        "        # padding\n",
        "        _, _, D, H, W = x.size()\n",
        "        if W % self.patch_size[2] != 0:\n",
        "            x = F.pad(x, (0, self.patch_size[2] - W % self.patch_size[2]))\n",
        "        if H % self.patch_size[1] != 0:\n",
        "            x = F.pad(x, (0, 0, 0, self.patch_size[1] - H % self.patch_size[1]))\n",
        "        if D % self.patch_size[0] != 0:\n",
        "            x = F.pad(x, (0, 0, 0, 0, 0, self.patch_size[0] - D % self.patch_size[0]))\n",
        "        x = self.proj(x)  # B C D Wh Ww\n",
        "        \n",
        "\n",
        "        if self.norm is not None:\n",
        "            D, Wh, Ww = x.size(2), x.size(3), x.size(4)\n",
        "            x = x.flatten(2).transpose(1,2)\n",
        "            print(x.shape)\n",
        "            x = self.norm(x)\n",
        "            print(x.shape)\n",
        "\n",
        "            x = x.transpose(1, 2).view(-1, self.embed_dim, D, Wh, Ww)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "8uY4xVA1ZK_H"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pathembed = PatchEmbed3D_pt(norm_layer = nn.LayerNorm)\n",
        "x_em_pt = pathembed(x_pt)\n",
        "x_em_pt.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIcw1gPdfqyn",
        "outputId": "55d511c2-601b-4e8d-ded4-863b6dc0fc6f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 256, 96])\n",
            "torch.Size([10, 256, 96])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 96, 4, 8, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbed_tf(tf.keras.layers.Layer):\n",
        "    def __init__(self, patch_size=(2, 4, 4), in_chans=3, embed_dim=96, norm_layer=None):\n",
        "        super().__init__(name='patch_embed')\n",
        "\n",
        "        \n",
        "        self.patch_size = patch_size\n",
        "        self.in_chans = in_chans\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        \n",
        "        self.proj = Conv3D(embed_dim, kernel_size=patch_size,\n",
        "                           strides=patch_size, name='proj', data_format = \"channels_first\")\n",
        "        \n",
        "        if norm_layer is not None:\n",
        "            self.norm = norm_layer(epsilon=1e-5, name='norm')\n",
        "        else:\n",
        "            self.norm = None\n",
        "\n",
        "    def call(self, x):\n",
        "        B, C, H, W, D = x.get_shape().as_list()\n",
        "        ## padding\n",
        "        x = self.proj(x)\n",
        "        print(x.get_shape())\n",
        "        if self.norm is not None:\n",
        "          B, C, D, Wh, Ww = x.get_shape().as_list()\n",
        "          x = tf.reshape(x, shape=[B, -1, C])\n",
        "          x = self.norm(x)\n",
        "          x = tf.reshape(x, shape=[B, C, -1])\n",
        "          x = tf.reshape(x, shape=[-1, self.embed_dim, D, Wh, Ww])\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "7QsHgyUAgttU"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "patchEmbed_tf = PatchEmbed_tf(norm_layer = LayerNormalization)\n",
        "x_em_tf = patchEmbed_tf(x_tf)\n",
        "x_em_tf.get_shape()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnBMVqmthN-a",
        "outputId": "e1dd3076-d3a0-46b7-a036-74d0bd4368f9"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 96, 4, 8, 8)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([10, 96, 4, 8, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array_equal(x_em_pt.detach().numpy(), x_em_tf.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOk2STVPhYU8",
        "outputId": "4f5d96d3-435d-4c84-ed1f-9c11f50b1435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_em_pt[0][0][0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szRTCblnp7qq",
        "outputId": "763e51f1-5dfb-4fa8-aeff-fed28122a776"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0364, -0.0411,  0.5812,  0.1997, -0.3046,  0.3152,  0.5739,  0.1697],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_em_tf[0][0][0][0]"
      ],
      "metadata": {
        "id": "FrIjJPKcqIfd",
        "outputId": "6b01cd73-643d-48b7-e1f7-36df06300432",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
              "array([-0.83596694, -0.39291137,  0.5411468 , -0.38081288, -0.69816816,\n",
              "        0.91106415, -1.7238436 , -0.8586185 ], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "afmogkVbqMUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP"
      ],
      "metadata": {
        "id": "jSmF87Peg6NJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mlp_pt(nn.Module):\n",
        "    \"\"\" Multilayer perceptron.\"\"\"\n",
        "\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "7IYnXOMAg68v"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mlp_tf(tf.keras.layers.Layer):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = Dense(hidden_features)\n",
        "        self.fc2 = Dense(out_features)\n",
        "        self.drop = Dropout(drop)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = tf.keras.activations.gelu(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Ni0OG87ahDPH"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4giEYsPlhzf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# window reverse"
      ],
      "metadata": {
        "id": "8v_vOwuoiInJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def window_reverse_pt(windows, window_size, B, D, H, W):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        windows: (B*num_windows, window_size, window_size, C)\n",
        "        window_size (tuple[int]): Window size\n",
        "        H (int): Height of image\n",
        "        W (int): Width of image\n",
        "    Returns:\n",
        "        x: (B, D, H, W, C)\n",
        "    \"\"\"\n",
        "    x = windows.view(B, D // window_size[0], H // window_size[1], W // window_size[2], window_size[0], window_size[1], window_size[2], -1)\n",
        "    x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(B, D, H, W, -1)\n",
        "    return "
      ],
      "metadata": {
        "id": "ulfFBtfTiH2g"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def window_reverse_tf(windows, window_size, H, W, C):\n",
        "    x = tf.reshape(B, shape=[ D // window_size[0], H // window_size[1], W // window_size[2], window_size[0], window_size[1], window_size[2], -1])\n",
        "    x = tf.transpose(x, perm=[0, 1, 4, 2, 5, 3, 6, 7])\n",
        "    x = tf.reshape(x, shape=[B, D, H, W, -1])\n",
        "    return x"
      ],
      "metadata": {
        "id": "mUDplFnziKzh"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XDRiIoLfjfc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WindowAttention3D"
      ],
      "metadata": {
        "id": "XQfX1w3RkIe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_pt = torch.rand(100,8,32,32,3)\n",
        "x_np = x_pt.numpy()\n",
        "x_tf = tf.convert_to_tensor(x_np)\n",
        "\n",
        "x_pt.shape , x_tf.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPDvgpN6mE0E",
        "outputId": "4cf16aa7-47b8-41cf-dcc7-43085dc57e7c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([100, 8, 32, 32, 3]), TensorShape([100, 8, 32, 32, 3]))"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WindowAttention3D_pt(nn.Module):\n",
        "    \"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
        "    It supports both of shifted and non-shifted window.\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        window_size (tuple[int]): The temporal length, height and width of the window.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
        "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
        "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, window_size, num_heads, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size  # Wd, Wh, Ww\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        # define a parameter table of relative position bias\n",
        "        self.relative_position_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1) * (2 * window_size[2] - 1), num_heads))  # 2*Wd-1 * 2*Wh-1 * 2*Ww-1, nH\n",
        "\n",
        "        # get pair-wise relative position index for each token inside the window\n",
        "        coords_d = torch.arange(self.window_size[0])\n",
        "        coords_h = torch.arange(self.window_size[1])\n",
        "        coords_w = torch.arange(self.window_size[2])\n",
        "        coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w))  # 3, Wd, Wh, Ww\n",
        "        coords_flatten = torch.flatten(coords, 1)  # 3, Wd*Wh*Ww\n",
        "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 3, Wd*Wh*Ww, Wd*Wh*Ww\n",
        "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wd*Wh*Ww, Wd*Wh*Ww, 3\n",
        "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
        "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
        "        relative_coords[:, :, 2] += self.window_size[2] - 1\n",
        "\n",
        "        relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)\n",
        "        relative_coords[:, :, 1] *= (2 * self.window_size[2] - 1)\n",
        "        relative_position_index = relative_coords.sum(-1)  # Wd*Wh*Ww, Wd*Wh*Ww\n",
        "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\" Forward function.\n",
        "        Args:\n",
        "            x: input features with shape of (num_windows*B, N, C)\n",
        "            mask: (0/-inf) mask with shape of (num_windows, N, N) or None\n",
        "        \"\"\"\n",
        "        B_, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]  # B_, nH, N, C\n",
        "\n",
        "        q = q * self.scale\n",
        "        attn = q @ k.transpose(-2, -1)\n",
        "\n",
        "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index[:N, :N].reshape(-1)].reshape(\n",
        "            N, N, -1)  # Wd*Wh*Ww,Wd*Wh*Ww,nH\n",
        "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wd*Wh*Ww, Wd*Wh*Ww\n",
        "        attn = attn + relative_position_bias.unsqueeze(0) # B_, nH, N, N\n",
        "\n",
        "        if mask is not None:\n",
        "            nW = mask.shape[0]\n",
        "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
        "            attn = attn.view(-1, self.num_heads, N, N)\n",
        "            attn = self.softmax(attn)\n",
        "        else:\n",
        "            attn = self.softmax(attn)\n",
        "\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "I-5g0vtukHWe"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WindowAttention3D_tf(tf.keras.layers.Layer):\n",
        "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "\n",
        "        self.qkv = Dense(dim * 3, use_bias=qkv_bias)\n",
        "        self.attn_drop = Dropout(attn_drop)\n",
        "        self.proj = Dense(dim)\n",
        "        self.proj_drop = Dropout(proj_drop)\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.relative_position_bias_table = self.add_weight(shape=((2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1)* (2 * window_size[2] - 1), self.num_heads),\n",
        "                                                            initializer=tf.initializers.Zeros(), trainable=True) # 2*Wd-1 * 2*Wh-1 * 2*Ww-1, nH\n",
        "\n",
        "        coords_d = np.arange(self.window_size[0])\n",
        "        coords_h = np.arange(self.window_size[1])\n",
        "        coords_w = np.arange(self.window_size[2])\n",
        "        coords = np.stack(np.meshgrid(coords_d, coords_h, coords_w, indexing='ij')) # 3, Wd, Wh, Ww\n",
        "        coords_flatten = coords.reshape(3, -1)\n",
        "        relative_coords = coords_flatten[:, :,\n",
        "                                         None] - coords_flatten[:, None, :]\n",
        "        relative_coords = relative_coords.transpose([1, 2, 0])\n",
        "        relative_coords[:, :, 0] += self.window_size[0] - 1\n",
        "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
        "        relative_coords[:, :, 2] += self.window_size[2] - 1\n",
        "\n",
        "        relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)\n",
        "        relative_coords[:, :, 1] *= (2 * self.window_size[2] - 1)\n",
        "        relative_position_index = relative_coords.sum(-1).astype(np.int64)\n",
        "        \n",
        "\n",
        "        self.relative_position_index = tf.Variable(initial_value=tf.convert_to_tensor(\n",
        "            relative_position_index), trainable=False)\n",
        "        self.built = True\n",
        "\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        B_, N, C = x.get_shape().as_list()\n",
        "        qkv = tf.transpose(tf.reshape(self.qkv(\n",
        "            x), shape=[-1, N, 3, self.num_heads, C // self.num_heads]), perm=[2, 0, 3, 1, 4])\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        q = q * self.scale\n",
        "        attn = (q @ tf.transpose(k, perm=[0, 1, 3, 2]))\n",
        "        index = tf.reshape(self.relative_position_index[:N, :N], shape=[-1])\n",
        "        relative_position_bias = tf.gather(self.relative_position_bias_table, index)\n",
        "        relative_position_bias = tf.reshape(relative_position_bias, shape=[N, N, -1])\n",
        "        relative_position_bias = tf.transpose(relative_position_bias, perm=[2, 0, 1])\n",
        "        attn = attn + tf.expand_dims(relative_position_bias, axis=0)\n",
        "\n",
        "        if mask is not None:\n",
        "            nW = mask.get_shape()[0]  # tf.shape(mask)[0]\n",
        "            attn = tf.reshape(attn, shape=[-1, nW, self.num_heads, N, N]) + tf.cast(\n",
        "                tf.expand_dims(tf.expand_dims(mask, axis=1), axis=0), attn.dtype)\n",
        "            attn = tf.reshape(attn, shape=[-1, self.num_heads, N, N])\n",
        "            attn = tf.nn.softmax(attn, axis=-1)\n",
        "        else:\n",
        "            attn = tf.nn.softmax(attn, axis=-1)\n",
        "\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = tf.transpose((attn @ v), perm=[0, 2, 1, 3])\n",
        "        x = tf.reshape(x, shape=[-1, N, C])\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "2rmrq6VKkUOQ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "atten_tf = WindowAttention3D_tf(3, window_size=(2,7, 7), num_heads=3)\n",
        "wp_tf = window_partition_tf(x_tf, (4,4,4))\n",
        "a_tf = atten_tf(wp_tf)\n",
        "a_tf.get_shape()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHctbbvqabrJ",
        "outputId": "aa7d2eaf-ea73-46ec-8350-41d704baf09b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([12800, 64, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_pt = WindowAttention3D_pt(3, window_size=(2,7, 7), num_heads=3)\n",
        "wp_pt = window_partition_pt(x_pt, (4,4,4))\n",
        "a_pt = attention_pt(s)\n",
        "a_pt.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W75n_5rrahOh",
        "outputId": "e5599bb8-00ad-44ab-c697-358125efd0e2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12800, 64, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a_tf[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yga2VZyFfNEM",
        "outputId": "be3489a6-fc25-4ba7-b281-e07ec9d7c7c2"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([-0.3371957 , -0.07501692, -0.037497  ], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a_pt[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xtZgeLilopM",
        "outputId": "c8d4d39f-0fca-4234-eecd-36ceb4388bba"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1801, 0.2295, 0.7769], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MYzgmhSMd5kB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}