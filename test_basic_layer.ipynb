{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from VideoSwinTransformer import *\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_transpose(w):\n",
    "    return w.transpose(2,3,4,1, 0)\n",
    "    \n",
    "\n",
    "def modify_tf_block( tf_component, pt_weight,  pt_bias = None, is_attn=False):\n",
    "    in_shape = pt_weight.shape\n",
    "\n",
    "    if isinstance(tf_component, tf.keras.layers.Conv3D) :\n",
    "      pt_weight = conv_transpose(pt_weight)\n",
    "\n",
    "    if isinstance(tf_component, tf.keras.layers.Dense) and not is_attn:\n",
    "      pt_weight =pt_weight.transpose()\n",
    "\n",
    "    if isinstance(tf_component, (tf.keras.layers.Dense, tf.keras.layers.Conv3D)):\n",
    "        tf_component.kernel.assign(tf.Variable(pt_weight))\n",
    "        if pt_bias is not None:\n",
    "            tf_component.bias.assign(tf.Variable(pt_bias))\n",
    "\n",
    "    elif isinstance(tf_component, tf.keras.layers.LayerNormalization):\n",
    "\n",
    "        tf_component.gamma.assign(tf.Variable(pt_weight))\n",
    "\n",
    "        tf_component.beta.assign(tf.Variable(pt_bias))\n",
    "\n",
    "    elif isinstance(tf_component, (tf.Variable)):\n",
    "        tf_component.assign(tf.Variable(pt_weight))\n",
    "\n",
    "    else:\n",
    "        return tf.convert_to_tensor(pt_weight)\n",
    "        \n",
    "        \n",
    "\n",
    "    return tf_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x(shape=(1,3,8,224,224)):\n",
    "    x_pt = torch.rand(shape) * 255\n",
    "    x_np = x_pt.numpy()\n",
    "    x_tf = tf.convert_to_tensor(x_np)\n",
    "\n",
    "    return x_tf, x_pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tf, x_pt = get_x()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_method = model_configs.MODEL_MAP[\"swin_tiny_patch244_window877_kinetics400_1k\"]\n",
    "cfg = cfg_method()\n",
    "\n",
    "name = cfg[\"name\"]\n",
    "link = cfg['link']\n",
    "del cfg[\"name\"]\n",
    "del cfg['link']\n",
    "del cfg[\"drop_path_rate\"]\n",
    "# download_weight_command = f\"wget {link} -O {name}.pth\"\n",
    "# os.system(download_weight_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "-------pt\n",
      "-------tf\n"
     ]
    }
   ],
   "source": [
    "pt_model = SwinTransformer3D_pt(**cfg,drop_rate=0.4, drop_path_rate=0., isTest= True)\n",
    "print(\"--------\")\n",
    "tf_model = SwinTransformer3D(**cfg,drop_rate=0.4, drop_path_rate=0., isTest= True)\n",
    "x_tf, x_pt = get_x()\n",
    "\n",
    "\n",
    "print(\"-------pt\")\n",
    "basic_pt, z= pt_model(x_pt)\n",
    "print(\"-------tf\")\n",
    "\n",
    "basic_tf, y = tf_model(x_tf)\n",
    "checkpoint = torch.load(f'{name}.pth')\n",
    "\n",
    "\n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in checkpoint['state_dict'].items():\n",
    "    if 'backbone' in k:\n",
    "        nam = k[9:]\n",
    "        new_state_dict[nam] = v \n",
    "\n",
    "pt_model.load_state_dict(new_state_dict) \n",
    "pt_model.eval()\n",
    "\n",
    "#(64, 196, 96)  (64, 196, 196)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check basic4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<VideoSwinTransformer.BasicLayer.BasicLayer object at 0x000001A51A9C7FD0> 768 (2, 768, 2, 7, 7) 2 24 (8, 7, 7) 4.0 True None 0.4 0.0 [0.0, 0.0] <class 'keras.layers.normalization.layer_normalization.LayerNormalization'> None False\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### window_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from operator import mul\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_partition_pt(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, D, H, W, C)\n",
    "        window_size (tuple[int]): window size\n",
    "    Returns:\n",
    "        windows: (B*num_windows, window_size*window_size, C)\n",
    "    \"\"\"\n",
    "    print(x.size(), window_size)\n",
    "    B, D, H, W, C = x.shape\n",
    "    x = x.view(B, D // window_size[0], window_size[0], H // window_size[1], window_size[1], W // window_size[2], window_size[2], C)\n",
    "    windows = x.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, reduce(mul, window_size), C)\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tf, x_pt = get_x((1, 4, 56, 56, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition (1, 4, 56, 56, 96) (4, 7, 7)\n",
      "torch.Size([1, 4, 56, 56, 96]) (4, 7, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 196, 96]), torch.Size([64, 196, 96]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = window_partition(x_tf, (4,7,7))\n",
    "\n",
    "y = window_partition_pt(x_pt, (4,7,7))\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtol = 1e-4\n",
    "etol = 1e-4\n",
    "\n",
    "np.testing.assert_allclose(x.numpy(), y.detach().numpy(), etol, rtol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check WindowAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "\n",
    "class WindowAttention3D_pt(nn.Module):\n",
    "    \"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The temporal length, height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "        super().__init__()\n",
    "        print( dim, window_size, num_heads, qkv_bias, qk_scale, attn_drop, proj_drop)\n",
    "\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wd, Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1) * (2 * window_size[2] - 1), num_heads))  # 2*Wd-1 * 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_d = torch.arange(self.window_size[0])\n",
    "        coords_h = torch.arange(self.window_size[1])\n",
    "        coords_w = torch.arange(self.window_size[2])\n",
    "        coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w))  # 3, Wd, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 3, Wd*Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 3, Wd*Wh*Ww, Wd*Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wd*Wh*Ww, Wd*Wh*Ww, 3\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 2] += self.window_size[2] - 1\n",
    "\n",
    "        relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)\n",
    "        relative_coords[:, :, 1] *= (2 * self.window_size[2] - 1)\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wd*Wh*Ww, Wd*Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\" Forward function.\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, N, N) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # B_, nH, N, C\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index[:N, :N].reshape(-1)].reshape(\n",
    "            N, N, -1)  # Wd*Wh*Ww,Wd*Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wd*Wh*Ww, Wd*Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0) # B_, nH, N, N\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 (8, 7, 7) 3 True None 0.0 0.4\n"
     ]
    }
   ],
   "source": [
    "atten_tf = WindowAttention3D(96, (8, 7, 7), 3, True, None, 0.0, 0.4)\n",
    "atten_pt = WindowAttention3D_pt(96, (8, 7, 7), 3, True, None, 0.0, 0.4)\n",
    "\n",
    "x_tf, x_pt = get_x((64, 196, 96))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute mask pt\n",
      "torch.Size([1, 4, 56, 56, 1]) (4, 7, 7)\n",
      "compute mask cm tf\n",
      "partition (1, 4, 56, 56, 1) (4, 7, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 196, 196]), torch.Size([64, 196, 196]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from VideoSwinTransformer.SwinTransformer3D_pt import compute_mask\n",
    "\n",
    "\n",
    "mask_pt = compute_mask(4,56,56, (4,7,7), (0,3,3), None)\n",
    "\n",
    "\n",
    "from VideoSwinTransformer import compute_mask\n",
    "\n",
    "\n",
    "mask_tf = compute_mask(4,56,56, (4,7,7), (0,3,3))\n",
    "mask_tf.shape , mask_pt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 1, 10), dtype=float32, numpy=\n",
       " array([[[ 73.772285,  64.29088 , -50.178078,   4.166691,  -7.296245,\n",
       "           52.691998, 111.41441 ,  56.152546,  20.024395, -20.882938]]],\n",
       "       dtype=float32)>,\n",
       " tensor([[[  13.6281,  -74.2625,   43.2334,  -21.4849, -141.7670,  -80.8282,\n",
       "             11.9369,   48.6482,  -66.5065,  -21.7590]]],\n",
       "        grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = atten_tf(x_tf, mask_tf)\n",
    "y = atten_pt(x_pt, mask_pt)\n",
    "x[:1,:1,:10] , y[:1,:1,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['relative_position_bias_table', 'relative_position_index', 'qkv.weight', 'qkv.bias', 'proj.weight', 'proj.bias'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten_pt.eval()\n",
    "np_state_dict = atten_pt.state_dict()\n",
    "np_state_dict = {k: np_state_dict[k].numpy() for k in np_state_dict}\n",
    "np_state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_atten_block(inner_layer, np_state_dict):\n",
    "\n",
    "\n",
    "    # Relative position.\n",
    "    inner_layer.relative_position_bias_table = (\n",
    "        modify_tf_block(\n",
    "            inner_layer.relative_position_bias_table,\n",
    "            np_state_dict[\n",
    "                f\"relative_position_bias_table\"\n",
    "            ] \n",
    "        )\n",
    "    )\n",
    "    inner_layer.relative_position_index = (\n",
    "        modify_tf_block(\n",
    "            inner_layer.relative_position_index,\n",
    "            np_state_dict[\n",
    "                f\"relative_position_index\"\n",
    "            ]\n",
    "\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # QKV.\n",
    "    inner_layer.qkv = modify_tf_block(\n",
    "        inner_layer.qkv,\n",
    "        np_state_dict[f\"qkv.weight\"],\n",
    "        np_state_dict[f\"qkv.bias\"],\n",
    "\n",
    "    )\n",
    "\n",
    "    # Projection.\n",
    "    inner_layer.proj = modify_tf_block(\n",
    "        inner_layer.proj,\n",
    "        np_state_dict[f\"proj.weight\"],\n",
    "        np_state_dict[f\"proj.bias\"],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = modify_atten_block(atten_tf, np_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 1, 10), dtype=float32, numpy=\n",
       " array([[[  8.176866, -44.557507,  25.940012, -12.890912, -85.06018 ,\n",
       "          -48.496895,   7.162156,  29.188894, -39.903904, -13.055373]]],\n",
       "       dtype=float32)>,\n",
       " tensor([[[  8.1769, -44.5575,  25.9400, -12.8909, -85.0602, -48.4969,   7.1622,\n",
       "            29.1889, -39.9039, -13.0554]]], grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = atten_tf(x_tf, mask_tf)\n",
    "y = atten_pt(x_pt, mask_pt)\n",
    "x[:1,:1,:10] , y[:1,:1,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtol = 1e-4\n",
    "etol = 1e-4\n",
    "\n",
    "np.testing.assert_allclose(x.numpy(), y.detach().numpy(), etol, rtol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check roll (passed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 4, 56, 56, 96]), torch.Size([1, 4, 56, 56, 96]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tf, x_pt = get_x((1, 4, 56, 56, 96))\n",
    "shift_size = (4,3,3)\n",
    "\n",
    "shifted_x_tf = tf.roll(x_tf, shift=[-shift_size[0], -shift_size[1], -shift_size[2]], axis=[1, 2, 3]) #?\n",
    "\n",
    "shifted_x_pt = torch.roll(x_pt, shifts=(-shift_size[0], -shift_size[1], -shift_size[2]), dims=(1, 2, 3))\n",
    "shifted_x_tf.shape, shifted_x_pt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtol = 1e-6\n",
    "etol = 1e-6\n",
    "\n",
    "np.testing.assert_allclose(shifted_x_tf.numpy(), shifted_x_pt.detach().numpy(), etol, rtol)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check identity (passed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tf, x_pt = get_x((1, 4, 56, 56, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 4, 56, 56, 96]), torch.Size([1, 4, 56, 56, 96]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x = tf.identity(x_tf)\n",
    "\n",
    "drop_path_pt = nn.Identity()\n",
    "y = drop_path_pt(x_pt)\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtol = 1e-7\n",
    "etol = 1e-7\n",
    "np.testing.assert_allclose(x.numpy(), y.detach().numpy(), etol, rtol)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check mlp (passed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp_pt(nn.Module):\n",
    "    \"\"\" Multilayer perceptron.\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, hidden_features=None, act_layer=nn.GELU, out_features=None,  drop=0.):\n",
    "        super().__init__()\n",
    "        print(in_features, hidden_features, act_layer,  out_features, drop)\n",
    "\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        # print('dense',hidden_features, in_features)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"mlp\", x.shape)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tf, x_pt = get_x((1, 4, 56, 56, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "96 384 <class 'torch.nn.modules.activation.GELU'> None 0.4\n"
     ]
    }
   ],
   "source": [
    "mlp_tf = mlp_block(96, 384 , tf.keras.activations.gelu ,None, 0.4)\n",
    "print()\n",
    "mlp_pt = Mlp_pt(96, 384 , nn.GELU ,None, 0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp torch.Size([1, 4, 56, 56, 96])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 1, 1, 1, 10), dtype=float32, numpy=\n",
       " array([[[[[ -55.042526 ,   30.78402  , -280.38745  ,  -72.948235 ,\n",
       "             -39.252743 ,  -13.429337 ,   -5.7349243, -161.74384  ,\n",
       "             -73.916504 ,   24.173918 ]]]]], dtype=float32)>,\n",
       " tensor([[[[[-125.6045,    0.0000,   77.3646,  124.4250,  -21.5418,  -22.6836,\n",
       "               50.7710,   63.3386,   21.1723,   -0.0000]]]]],\n",
       "        grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = mlp_tf(x_tf)\n",
    "y = mlp_pt(x_pt)\n",
    "\n",
    "x[:1,:1,:1,:1,:10], y[:1,:1,:1,:1,:10] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_pt.eval()\n",
    "np_state_dict = mlp_pt.state_dict()\n",
    "np_state_dict = {k: np_state_dict[k].numpy() for k in np_state_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_mlp(inner_layer, np_state_dict):\n",
    "    mlp_layer_idx = 1\n",
    "    for mlp_layer in inner_layer.layers:\n",
    "\n",
    "        if isinstance(mlp_layer, tf.keras.layers.Dense):\n",
    "            mlp_layer = modify_tf_block(\n",
    "                mlp_layer,\n",
    "                np_state_dict[\n",
    "                    f\"fc{mlp_layer_idx}.weight\"\n",
    "                ],\n",
    "                np_state_dict[\n",
    "                    f\"fc{mlp_layer_idx}.bias\"\n",
    "                ]\n",
    "\n",
    "            )\n",
    "            mlp_layer_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = modify_mlp(mlp_tf, np_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp torch.Size([1, 4, 56, 56, 96])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 1, 1, 1, 10), dtype=float32, numpy=\n",
       " array([[[[[-10.9844885,   9.016938 , -46.57778  ,  45.277203 ,\n",
       "             16.67046  ,   2.3475373,  -6.9900846,   2.1323876,\n",
       "            -10.313778 , -48.950424 ]]]]], dtype=float32)>,\n",
       " tensor([[[[[-10.9845,   9.0169, -46.5778,  45.2772,  16.6705,   2.3475,\n",
       "              -6.9901,   2.1324, -10.3138, -48.9504]]]]],\n",
       "        grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = mlp_tf(x_tf)\n",
    "y = mlp_pt(x_pt)\n",
    "\n",
    "x[:1,:1,:1,:1,:10], y[:1,:1,:1,:1,:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtol = 1e-2\n",
    "etol = 1e-2\n",
    "\n",
    "np.testing.assert_allclose(x.numpy(), y.detach().numpy(), etol, rtol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
