{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install  -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "from VideoSwinTransformer import *\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = tf.keras.models.load_model('/Users/mohammadshoaib/Codes/tensorflow-test/GSOC-22-Video-Swin-Transformers/swin_tiny_patch244_window877_kinetics400_1k_tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_function = tf.function(lambda inputs: tf_model(inputs),\n",
    "                               [tf.TensorSpec(tf.TensorShape([None, 3, 8 ,224, 224]),dtype=tf.float64,\n",
    "    name=\"x\")])\n",
    "\n",
    "# network_function = tf.function(lambda inputs: tf_model(inputs),\n",
    "#                                [tf.TensorSpec(tf.keras.Input((3,8,224,224), dtype=tf.float64))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = list(map(lambda tname: network_function.get_concrete_function().graph.get_tensor_by_name(tname), [\n",
    "    \"conv_projection\"\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 18:34:05.460084: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-08-14 18:34:05.460561: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 8, 224, 224, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pt = torch.rand( (2,3, 8,224,224)) * 255\n",
    "x_np = x_pt.numpy()\n",
    "x_tf = tf.convert_to_tensor(x_np)\n",
    "x_tf = tf.transpose(x_tf, perm=(0,2,3,4,1))\n",
    "x_tf = tf.cast(x_tf, dtype= tf.float32)\n",
    "\n",
    "x_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape:  (2, 768, 2, 7, 7)\n",
      "KerasTensor(type_spec=TensorSpec(shape=(2, 3, 8, 224, 224), dtype=tf.float32, name=None), description=\"created by layer 'input_2'\")\n"
     ]
    }
   ],
   "source": [
    "from VideoSwinTransformer import model_configs , SwinTransformer3D\n",
    "swin = SwinTransformer3D()\n",
    "\n",
    "input_shape = (2,3, 8,224, 224)\n",
    "x = tf.keras.Input((8,224,224,3))\n",
    "swin = SwinTransformer3D(shape_of_input = x.shape)\n",
    "x = tf.random.normal(input_shape,   dtype=\"float32\")\n",
    "# swin = SwinTransformer3D(x)\n",
    "x  = tf.keras.layers.Input(tensor=x)\n",
    "\n",
    "output = swin(x, training= False)\n",
    "print(\"output shape: \",output.shape)\n",
    "# print(swin.get_layer(\"basic_layer\").layers)\n",
    "\n",
    "print(swin.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 3, 8, 224, 224)]  0         \n",
      "                                                                 \n",
      " tf.compat.v1.transpose (TFO  (None, 8, 224, 224, 3)   0         \n",
      " pLambda)                                                        \n",
      "                                                                 \n",
      " projection (Sequential)     (None, 2, 56, 56, 96)     18528     \n",
      "                                                                 \n",
      " tf.compat.v1.transpose_1 (T  (None, 96, 2, 56, 56)    0         \n",
      " FOpLambda)                                                      \n",
      "                                                                 \n",
      " dropout_49 (Dropout)        (None, 96, 2, 56, 56)     0         \n",
      "                                                                 \n",
      " basic_layer_4 (BasicLayer)  (None, 192, 2, 28, 28)    320426    \n",
      "                                                                 \n",
      " basic_layer_5 (BasicLayer)  (None, 384, 2, 14, 14)    1211468   \n",
      "                                                                 \n",
      " basic_layer_6 (BasicLayer)  (None, 768, 2, 7, 7)      11923632  \n",
      "                                                                 \n",
      " basic_layer_7 (BasicLayer)  (None, None, 2, 7, 7)     14219288  \n",
      "                                                                 \n",
      " tf.compat.v1.transpose_2 (T  (None, 2, 7, 7, None)    0         \n",
      " FOpLambda)                                                      \n",
      "                                                                 \n",
      " layer_normalization_55 (Lay  (None, 2, 7, 7, 768)     1536      \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " tf.compat.v1.transpose_3 (T  (None, 768, 2, 7, 7)     0         \n",
      " FOpLambda)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,694,878\n",
      "Trainable params: 27,579,630\n",
      "Non-trainable params: 115,248\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "swin.build_graph().summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydot\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting graphviz\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m738.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.1.4 in /Users/mohammadshoaib/Codes/tensorflow-test/env/lib/python3.8/site-packages (from pydot) (3.0.9)\n",
      "Installing collected packages: pydot, graphviz\n",
      "Successfully installed graphviz-0.20.1 pydot-1.4.2\n"
     ]
    }
   ],
   "source": [
    "! pip install pydot graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "# Just showing all possible argument for newcomer.  \n",
    "tf.keras.utils.plot_model(\n",
    "    swin.build_graph(),                      # here is the trick (for now)\n",
    "             # saving  \n",
    "    show_shapes=True, show_layer_names=True,  # show shapes and layer name\n",
    "    expand_nested=False                       # will show nested block\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_method = model_configs.MODEL_MAP[\"swin_tiny_patch244_window877_kinetics400_1k\"]\n",
    "cfg = cfg_method()\n",
    "\n",
    "name = cfg[\"name\"]\n",
    "link = cfg['link']\n",
    "del cfg[\"name\"]\n",
    "del cfg['link']\n",
    "\n",
    "pt_model = SwinTransformer3D_pt(**cfg)\n",
    "\n",
    "checkpoint = torch.load(f'{name}.pth')\n",
    "\n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in checkpoint['state_dict'].items():\n",
    "    if 'backbone' in k:\n",
    "        name = k[9:]\n",
    "        new_state_dict[name] = v \n",
    "\n",
    "pt_model.load_state_dict(new_state_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf_model.predict(x_tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = pt_model(x_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.round(y, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "z.shape , y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_state_dict = pt_model.state_dict()\n",
    "np_state_dict = {k: np_state_dict[k].numpy() for k in np_state_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(np_state_dict['patch_embed.proj.bias'] ,tf_model.projection.layers[0].bias.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for weight in tf_model.weights:\n",
    "    i +=1\n",
    "    if \"dense\" in weight.name:\n",
    "        \n",
    "        isEqual = np.array_equal(weight.numpy(),np_state_dict[tf_pt[weight.name]].transpose() )\n",
    "    else:\n",
    "        isEqual = np.array_equal(weight.numpy(),np_state_dict[tf_pt[weight.name]] )\n",
    "\n",
    "    if not isEqual:\n",
    "        print(weight.name, weight.numpy().shape , np_state_dict[tf_pt[weight.name]].shape )\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "tf_pt = {}\n",
    "  \n",
    "# Opening JSON file\n",
    "f = open('data.json')\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "f.close()\n",
    "# Iterating through the json\n",
    "# list\n",
    "for idx, layer in enumerate(data):\n",
    "    # print(idx, layer, \"------\" ,  data[layer])\n",
    "    tf_pt[data[layer]] = layer\n",
    "  \n",
    "# Closing file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
       "array([1, 3, 2, 2, 4, 2, 4, 4, 4, 4, 3, 4, 4, 3, 1, 1, 0, 0, 3, 4, 4, 0,\n",
       "       0, 2, 1, 3, 2, 2, 2, 1, 0, 4, 4, 3, 4, 1, 4, 0, 0, 4, 0, 2, 4, 4,\n",
       "       3, 0, 1, 1, 1, 1, 2, 2, 1, 3, 0, 3, 1, 1, 1, 4, 4, 1, 2, 0, 2, 0,\n",
       "       1, 3, 3, 2, 4, 0, 4, 1, 4, 0, 0, 1, 2, 0, 0, 0, 4, 3, 3, 1, 1, 0,\n",
       "       4, 0, 4, 2, 3, 4, 4, 3, 0, 0, 4, 0])>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.normal([100,3,8, 224,224])\n",
    "y = tf.random.uniform(shape=[100], minval=0, maxval=5, dtype='int64')\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "swin_transformer_block3d_24 (10, 2, 56, 56, 96)\n",
      "swin_transformer_block3d_24 (10, 2, 56, 56, 96)\n",
      "swin_transformer_block3d_25 (10, 2, 56, 56, 96)\n",
      "swin_transformer_block3d_24 (10, 2, 56, 56, 96)\n",
      "swin_transformer_block3d_24 (10, 2, 56, 56, 96)\n",
      "swin_transformer_block3d_25 (10, 2, 56, 56, 96)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/mohammadshoaib/Codes/tensorflow-test/env/lib/python3.8/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/mohammadshoaib/Codes/tensorflow-test/env/lib/python3.8/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/mohammadshoaib/Codes/tensorflow-test/env/lib/python3.8/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/mohammadshoaib/Codes/tensorflow-test/env/lib/python3.8/site-packages/keras/engine/training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/mohammadshoaib/Codes/tensorflow-test/env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filejij3gzaf.py\", line 27, in tf__call\n        ag__.for_stmt(ag__.ld(self).layers3D, None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'layer'})\n    File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filejij3gzaf.py\", line 25, in loop_body\n        x = ag__.converted_call(ag__.ld(layer), (ag__.ld(x),), None, fscope)\n    File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_file2r_aahdq.py\", line 27, in tf__call\n        ag__.for_stmt(ag__.ld(self).blocks, None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'blk'})\n    File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_file2r_aahdq.py\", line 24, in loop_body\n        x = ag__.converted_call(ag__.ld(blk), (ag__.ld(x),), None, fscope)\n    File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filehjxj0sr4.py\", line 14, in tf__call\n        x = (ag__.ld(shortcut) + ag__.converted_call(ag__.ld(self).drop_path, (ag__.ld(x),), None, fscope))\n    File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_fileu4q22g41.py\", line 12, in tf__call\n        retval_ = ag__.converted_call(ag__.ld(drop_path), (ag__.ld(x), ag__.ld(self).drop_prob, ag__.ld(training)), None, fscope)\n    File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filemtrkf0q7.py\", line 47, in tf__drop_path\n        ag__.if_stmt(ag__.or_((lambda : ag__.not_(ag__.ld(is_training))), (lambda : (ag__.ld(drop_prob) == 0.0))), if_body, else_body, get_state, set_state, ('do_return', 'retval_'), 2)\n    File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filemtrkf0q7.py\", line 35, in else_body\n        output = (ag__.converted_call(ag__.ld(tf).math.divide, (ag__.ld(inputs), ag__.ld(keep_prob)), None, fscope) * ag__.ld(binary_tensor))\n\n    ValueError: Exception encountered when calling layer \"swin_transformer3d_2\" (type SwinTransformer3D).\n    \n    in user code:\n    \n        File \"/Users/mohammadshoaib/Codes/tensorflow-test/GSOC-22-Video-Swin-Transformers/VideoSwinTransformer/SwinTransformer3D.py\", line 131, in call  *\n            x = layer(x)\n        File \"/Users/mohammadshoaib/Codes/tensorflow-test/env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_file2r_aahdq.py\", line 27, in tf__call\n            ag__.for_stmt(ag__.ld(self).blocks, None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'blk'})\n        File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_file2r_aahdq.py\", line 24, in loop_body\n            x = ag__.converted_call(ag__.ld(blk), (ag__.ld(x),), None, fscope)\n        File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filehjxj0sr4.py\", line 14, in tf__call\n            x = (ag__.ld(shortcut) + ag__.converted_call(ag__.ld(self).drop_path, (ag__.ld(x),), None, fscope))\n        File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_fileu4q22g41.py\", line 12, in tf__call\n            retval_ = ag__.converted_call(ag__.ld(drop_path), (ag__.ld(x), ag__.ld(self).drop_prob, ag__.ld(training)), None, fscope)\n        File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filemtrkf0q7.py\", line 47, in tf__drop_path\n            ag__.if_stmt(ag__.or_((lambda : ag__.not_(ag__.ld(is_training))), (lambda : (ag__.ld(drop_prob) == 0.0))), if_body, else_body, get_state, set_state, ('do_return', 'retval_'), 2)\n        File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filemtrkf0q7.py\", line 35, in else_body\n            output = (ag__.converted_call(ag__.ld(tf).math.divide, (ag__.ld(inputs), ag__.ld(keep_prob)), None, fscope) * ag__.ld(binary_tensor))\n    \n        ValueError: Exception encountered when calling layer \"basic_layer_8\" (type BasicLayer).\n        \n        in user code:\n        \n            File \"/Users/mohammadshoaib/Codes/tensorflow-test/GSOC-22-Video-Swin-Transformers/VideoSwinTransformer/BasicLayer.py\", line 100, in call  *\n                x = blk(x)\n            File \"/Users/mohammadshoaib/Codes/tensorflow-test/env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filehjxj0sr4.py\", line 14, in tf__call\n                x = (ag__.ld(shortcut) + ag__.converted_call(ag__.ld(self).drop_path, (ag__.ld(x),), None, fscope))\n            File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_fileu4q22g41.py\", line 12, in tf__call\n                retval_ = ag__.converted_call(ag__.ld(drop_path), (ag__.ld(x), ag__.ld(self).drop_prob, ag__.ld(training)), None, fscope)\n            File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filemtrkf0q7.py\", line 47, in tf__drop_path\n                ag__.if_stmt(ag__.or_((lambda : ag__.not_(ag__.ld(is_training))), (lambda : (ag__.ld(drop_prob) == 0.0))), if_body, else_body, get_state, set_state, ('do_return', 'retval_'), 2)\n            File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filemtrkf0q7.py\", line 35, in else_body\n                output = (ag__.converted_call(ag__.ld(tf).math.divide, (ag__.ld(inputs), ag__.ld(keep_prob)), None, fscope) * ag__.ld(binary_tensor))\n        \n            ValueError: Exception encountered when calling layer \"swin_transformer_block3d_25\" (type SwinTransformerBlock3D).\n            \n            in user code:\n            \n                File \"/Users/mohammadshoaib/Codes/tensorflow-test/GSOC-22-Video-Swin-Transformers/VideoSwinTransformer/SwinTransformerBlock3D.py\", line 170, in call  *\n                    x = shortcut + self.drop_path(x)\n                File \"/Users/mohammadshoaib/Codes/tensorflow-test/env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n                File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_fileu4q22g41.py\", line 12, in tf__call\n                    retval_ = ag__.converted_call(ag__.ld(drop_path), (ag__.ld(x), ag__.ld(self).drop_prob, ag__.ld(training)), None, fscope)\n                File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filemtrkf0q7.py\", line 47, in tf__drop_path\n                    ag__.if_stmt(ag__.or_((lambda : ag__.not_(ag__.ld(is_training))), (lambda : (ag__.ld(drop_prob) == 0.0))), if_body, else_body, get_state, set_state, ('do_return', 'retval_'), 2)\n                File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filemtrkf0q7.py\", line 35, in else_body\n                    output = (ag__.converted_call(ag__.ld(tf).math.divide, (ag__.ld(inputs), ag__.ld(keep_prob)), None, fscope) * ag__.ld(binary_tensor))\n            \n                ValueError: Exception encountered when calling layer \"drop_path_22\" (type DropPath).\n                \n                in user code:\n                \n                    File \"/Users/mohammadshoaib/Codes/tensorflow-test/GSOC-22-Video-Swin-Transformers/VideoSwinTransformer/DropPath.py\", line 25, in call  *\n                        return drop_path(x, self.drop_prob, training)\n                    File \"/Users/mohammadshoaib/Codes/tensorflow-test/GSOC-22-Video-Swin-Transformers/VideoSwinTransformer/DropPath.py\", line 14, in drop_path  *\n                        output = tf.math.divide(inputs, keep_prob) * binary_tensor\n                \n                    ValueError: Dimensions must be equal, but are 96 and 14 for '{{node sequential_2/swin_transformer3d_2/basic_layer_8/swin_transformer_block3d_25/drop_path_22/mul_1}} = Mul[T=DT_FLOAT](sequential_2/swin_transformer3d_2/basic_layer_8/swin_transformer_block3d_25/drop_path_22/truediv, sequential_2/swin_transformer3d_2/basic_layer_8/swin_transformer_block3d_25/drop_path_22/Floor)' with input shapes: [10,2,56,56,96], [14].\n                \n                \n                Call arguments received by layer \"drop_path_22\" (type DropPath):\n                  • x=tf.Tensor(shape=(10, 2, 56, 56, 96), dtype=float32)\n                  • training=True\n            \n            \n            Call arguments received by layer \"swin_transformer_block3d_25\" (type SwinTransformerBlock3D):\n              • x=tf.Tensor(shape=(10, 2, 56, 56, 96), dtype=float32)\n        \n        \n        Call arguments received by layer \"basic_layer_8\" (type BasicLayer):\n          • x=tf.Tensor(shape=(10, 96, 2, 56, 56), dtype=float32)\n    \n    \n    Call arguments received by layer \"swin_transformer3d_2\" (type SwinTransformer3D):\n      • x=tf.Tensor(shape=(10, 3, 8, 224, 224), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/mohammadshoaib/Codes/tensorflow-test/GSOC-22-Video-Swin-Transformers/test.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mohammadshoaib/Codes/tensorflow-test/GSOC-22-Video-Swin-Transformers/test.ipynb#ch0000036?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msgd\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mohammadshoaib/Codes/tensorflow-test/GSOC-22-Video-Swin-Transformers/test.ipynb#ch0000036?line=4'>5</a>\u001b[0m \u001b[39m# This builds the model for the first time:\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mohammadshoaib/Codes/tensorflow-test/GSOC-22-Video-Swin-Transformers/test.ipynb#ch0000036?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(x, y, batch_size\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[0;32m~/Codes/tensorflow-test/env/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_file1f530caw.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filejij3gzaf.py:27\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m     x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(layer), (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     26\u001b[0m layer \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mlayer\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m ag__\u001b[39m.\u001b[39mfor_stmt(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mlayers3D, \u001b[39mNone\u001b[39;00m, loop_body, get_state, set_state, (\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m,), {\u001b[39m'\u001b[39m\u001b[39miterate_names\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mlayer\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[1;32m     28\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mtranspose, (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mdict\u001b[39m(perm\u001b[39m=\u001b[39m[\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m1\u001b[39m]), fscope)\n\u001b[1;32m     29\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mnorm, (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filejij3gzaf.py:25\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mnonlocal\u001b[39;00m x\n\u001b[1;32m     24\u001b[0m layer \u001b[39m=\u001b[39m itr\n\u001b[0;32m---> 25\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(layer), (ag__\u001b[39m.\u001b[39;49mld(x),), \u001b[39mNone\u001b[39;49;00m, fscope)\n",
      "File \u001b[0;32m/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_file2r_aahdq.py:27\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m     ag__\u001b[39m.\u001b[39mld(\u001b[39mprint\u001b[39m)(ag__\u001b[39m.\u001b[39mld(blk)\u001b[39m.\u001b[39mname, ag__\u001b[39m.\u001b[39mld(x)\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     26\u001b[0m blk \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mblk\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m ag__\u001b[39m.\u001b[39mfor_stmt(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mblocks, \u001b[39mNone\u001b[39;00m, loop_body, get_state, set_state, (\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m,), {\u001b[39m'\u001b[39m\u001b[39miterate_names\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mblk\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[1;32m     28\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mreshape, (ag__\u001b[39m.\u001b[39mld(x), [ag__\u001b[39m.\u001b[39mld(B), ag__\u001b[39m.\u001b[39mld(D), ag__\u001b[39m.\u001b[39mld(H), ag__\u001b[39m.\u001b[39mld(W), (\u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)]), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_1\u001b[39m():\n",
      "File \u001b[0;32m/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_file2r_aahdq.py:24\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mnonlocal\u001b[39;00m x\n\u001b[1;32m     23\u001b[0m blk \u001b[39m=\u001b[39m itr\n\u001b[0;32m---> 24\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(blk), (ag__\u001b[39m.\u001b[39;49mld(x),), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[1;32m     25\u001b[0m ag__\u001b[39m.\u001b[39mld(\u001b[39mprint\u001b[39m)(ag__\u001b[39m.\u001b[39mld(blk)\u001b[39m.\u001b[39mname, ag__\u001b[39m.\u001b[39mld(x)\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filehjxj0sr4.py:14\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m shortcut \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(x)\n\u001b[1;32m     13\u001b[0m x \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mforward_part1, (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 14\u001b[0m x \u001b[39m=\u001b[39m (ag__\u001b[39m.\u001b[39mld(shortcut) \u001b[39m+\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mdrop_path, (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mNone\u001b[39;00m, fscope))\n\u001b[1;32m     15\u001b[0m x \u001b[39m=\u001b[39m (ag__\u001b[39m.\u001b[39mld(x) \u001b[39m+\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mforward_part2, (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mNone\u001b[39;00m, fscope))\n\u001b[1;32m     16\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_fileu4q22g41.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, x, training)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(drop_path), (ag__\u001b[39m.\u001b[39mld(x), ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mdrop_prob, ag__\u001b[39m.\u001b[39mld(training)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filemtrkf0q7.py:47\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__drop_path\u001b[0;34m(inputs, drop_prob, is_training)\u001b[0m\n\u001b[1;32m     45\u001b[0m random_tensor \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mrandom_tensor\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     46\u001b[0m binary_tensor \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mbinary_tensor\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mor_((\u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mnot_(ag__\u001b[39m.\u001b[39mld(is_training))), (\u001b[39mlambda\u001b[39;00m : (ag__\u001b[39m.\u001b[39mld(drop_prob) \u001b[39m==\u001b[39m \u001b[39m0.0\u001b[39m))), if_body, else_body, get_state, set_state, (\u001b[39m'\u001b[39m\u001b[39mdo_return\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mretval_\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m2\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[39mreturn\u001b[39;00m fscope\u001b[39m.\u001b[39mret(retval_, do_return)\n",
      "File \u001b[0;32m/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filemtrkf0q7.py:35\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__drop_path.<locals>.else_body\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m random_tensor \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(tf\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39muniform, (shape,), \u001b[39mdict\u001b[39m(dtype\u001b[39m=\u001b[39minputs\u001b[39m.\u001b[39mdtype), fscope)\n\u001b[1;32m     34\u001b[0m binary_tensor \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mfloor, (ag__\u001b[39m.\u001b[39mld(random_tensor),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 35\u001b[0m output \u001b[39m=\u001b[39m (ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(tf)\u001b[39m.\u001b[39;49mmath\u001b[39m.\u001b[39;49mdivide, (ag__\u001b[39m.\u001b[39;49mld(inputs), ag__\u001b[39m.\u001b[39;49mld(keep_prob)), \u001b[39mNone\u001b[39;49;00m, fscope) \u001b[39m*\u001b[39;49m ag__\u001b[39m.\u001b[39;49mld(binary_tensor))\n\u001b[1;32m     36\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/mohammadshoaib/Codes/tensorflow-test/env/lib/python3.8/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/mohammadshoaib/Codes/tensorflow-test/env/lib/python3.8/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/mohammadshoaib/Codes/tensorflow-test/env/lib/python3.8/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/mohammadshoaib/Codes/tensorflow-test/env/lib/python3.8/site-packages/keras/engine/training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/mohammadshoaib/Codes/tensorflow-test/env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filejij3gzaf.py\", line 27, in tf__call\n        ag__.for_stmt(ag__.ld(self).layers3D, None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'layer'})\n    File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filejij3gzaf.py\", line 25, in loop_body\n        x = ag__.converted_call(ag__.ld(layer), (ag__.ld(x),), None, fscope)\n    File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_file2r_aahdq.py\", line 27, in tf__call\n        ag__.for_stmt(ag__.ld(self).blocks, None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'blk'})\n    File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_file2r_aahdq.py\", line 24, in loop_body\n        x = ag__.converted_call(ag__.ld(blk), (ag__.ld(x),), None, fscope)\n    File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filehjxj0sr4.py\", line 14, in tf__call\n        x = (ag__.ld(shortcut) + ag__.converted_call(ag__.ld(self).drop_path, (ag__.ld(x),), None, fscope))\n    File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_fileu4q22g41.py\", line 12, in tf__call\n        retval_ = ag__.converted_call(ag__.ld(drop_path), (ag__.ld(x), ag__.ld(self).drop_prob, ag__.ld(training)), None, fscope)\n    File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filemtrkf0q7.py\", line 47, in tf__drop_path\n        ag__.if_stmt(ag__.or_((lambda : ag__.not_(ag__.ld(is_training))), (lambda : (ag__.ld(drop_prob) == 0.0))), if_body, else_body, get_state, set_state, ('do_return', 'retval_'), 2)\n    File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filemtrkf0q7.py\", line 35, in else_body\n        output = (ag__.converted_call(ag__.ld(tf).math.divide, (ag__.ld(inputs), ag__.ld(keep_prob)), None, fscope) * ag__.ld(binary_tensor))\n\n    ValueError: Exception encountered when calling layer \"swin_transformer3d_2\" (type SwinTransformer3D).\n    \n    in user code:\n    \n        File \"/Users/mohammadshoaib/Codes/tensorflow-test/GSOC-22-Video-Swin-Transformers/VideoSwinTransformer/SwinTransformer3D.py\", line 131, in call  *\n            x = layer(x)\n        File \"/Users/mohammadshoaib/Codes/tensorflow-test/env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_file2r_aahdq.py\", line 27, in tf__call\n            ag__.for_stmt(ag__.ld(self).blocks, None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'blk'})\n        File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_file2r_aahdq.py\", line 24, in loop_body\n            x = ag__.converted_call(ag__.ld(blk), (ag__.ld(x),), None, fscope)\n        File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filehjxj0sr4.py\", line 14, in tf__call\n            x = (ag__.ld(shortcut) + ag__.converted_call(ag__.ld(self).drop_path, (ag__.ld(x),), None, fscope))\n        File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_fileu4q22g41.py\", line 12, in tf__call\n            retval_ = ag__.converted_call(ag__.ld(drop_path), (ag__.ld(x), ag__.ld(self).drop_prob, ag__.ld(training)), None, fscope)\n        File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filemtrkf0q7.py\", line 47, in tf__drop_path\n            ag__.if_stmt(ag__.or_((lambda : ag__.not_(ag__.ld(is_training))), (lambda : (ag__.ld(drop_prob) == 0.0))), if_body, else_body, get_state, set_state, ('do_return', 'retval_'), 2)\n        File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filemtrkf0q7.py\", line 35, in else_body\n            output = (ag__.converted_call(ag__.ld(tf).math.divide, (ag__.ld(inputs), ag__.ld(keep_prob)), None, fscope) * ag__.ld(binary_tensor))\n    \n        ValueError: Exception encountered when calling layer \"basic_layer_8\" (type BasicLayer).\n        \n        in user code:\n        \n            File \"/Users/mohammadshoaib/Codes/tensorflow-test/GSOC-22-Video-Swin-Transformers/VideoSwinTransformer/BasicLayer.py\", line 100, in call  *\n                x = blk(x)\n            File \"/Users/mohammadshoaib/Codes/tensorflow-test/env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filehjxj0sr4.py\", line 14, in tf__call\n                x = (ag__.ld(shortcut) + ag__.converted_call(ag__.ld(self).drop_path, (ag__.ld(x),), None, fscope))\n            File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_fileu4q22g41.py\", line 12, in tf__call\n                retval_ = ag__.converted_call(ag__.ld(drop_path), (ag__.ld(x), ag__.ld(self).drop_prob, ag__.ld(training)), None, fscope)\n            File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filemtrkf0q7.py\", line 47, in tf__drop_path\n                ag__.if_stmt(ag__.or_((lambda : ag__.not_(ag__.ld(is_training))), (lambda : (ag__.ld(drop_prob) == 0.0))), if_body, else_body, get_state, set_state, ('do_return', 'retval_'), 2)\n            File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filemtrkf0q7.py\", line 35, in else_body\n                output = (ag__.converted_call(ag__.ld(tf).math.divide, (ag__.ld(inputs), ag__.ld(keep_prob)), None, fscope) * ag__.ld(binary_tensor))\n        \n            ValueError: Exception encountered when calling layer \"swin_transformer_block3d_25\" (type SwinTransformerBlock3D).\n            \n            in user code:\n            \n                File \"/Users/mohammadshoaib/Codes/tensorflow-test/GSOC-22-Video-Swin-Transformers/VideoSwinTransformer/SwinTransformerBlock3D.py\", line 170, in call  *\n                    x = shortcut + self.drop_path(x)\n                File \"/Users/mohammadshoaib/Codes/tensorflow-test/env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n                File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_fileu4q22g41.py\", line 12, in tf__call\n                    retval_ = ag__.converted_call(ag__.ld(drop_path), (ag__.ld(x), ag__.ld(self).drop_prob, ag__.ld(training)), None, fscope)\n                File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filemtrkf0q7.py\", line 47, in tf__drop_path\n                    ag__.if_stmt(ag__.or_((lambda : ag__.not_(ag__.ld(is_training))), (lambda : (ag__.ld(drop_prob) == 0.0))), if_body, else_body, get_state, set_state, ('do_return', 'retval_'), 2)\n                File \"/var/folders/wm/qf7lbrys65zfbzv7f458sctm0000gn/T/__autograph_generated_filemtrkf0q7.py\", line 35, in else_body\n                    output = (ag__.converted_call(ag__.ld(tf).math.divide, (ag__.ld(inputs), ag__.ld(keep_prob)), None, fscope) * ag__.ld(binary_tensor))\n            \n                ValueError: Exception encountered when calling layer \"drop_path_22\" (type DropPath).\n                \n                in user code:\n                \n                    File \"/Users/mohammadshoaib/Codes/tensorflow-test/GSOC-22-Video-Swin-Transformers/VideoSwinTransformer/DropPath.py\", line 25, in call  *\n                        return drop_path(x, self.drop_prob, training)\n                    File \"/Users/mohammadshoaib/Codes/tensorflow-test/GSOC-22-Video-Swin-Transformers/VideoSwinTransformer/DropPath.py\", line 14, in drop_path  *\n                        output = tf.math.divide(inputs, keep_prob) * binary_tensor\n                \n                    ValueError: Dimensions must be equal, but are 96 and 14 for '{{node sequential_2/swin_transformer3d_2/basic_layer_8/swin_transformer_block3d_25/drop_path_22/mul_1}} = Mul[T=DT_FLOAT](sequential_2/swin_transformer3d_2/basic_layer_8/swin_transformer_block3d_25/drop_path_22/truediv, sequential_2/swin_transformer3d_2/basic_layer_8/swin_transformer_block3d_25/drop_path_22/Floor)' with input shapes: [10,2,56,56,96], [14].\n                \n                \n                Call arguments received by layer \"drop_path_22\" (type DropPath):\n                  • x=tf.Tensor(shape=(10, 2, 56, 56, 96), dtype=float32)\n                  • training=True\n            \n            \n            Call arguments received by layer \"swin_transformer_block3d_25\" (type SwinTransformerBlock3D):\n              • x=tf.Tensor(shape=(10, 2, 56, 56, 96), dtype=float32)\n        \n        \n        Call arguments received by layer \"basic_layer_8\" (type BasicLayer):\n          • x=tf.Tensor(shape=(10, 96, 2, 56, 56), dtype=float32)\n    \n    \n    Call arguments received by layer \"swin_transformer3d_2\" (type SwinTransformer3D):\n      • x=tf.Tensor(shape=(10, 3, 8, 224, 224), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(SwinTransformer3D())\n",
    "model.add(tf.keras.layers.Dense(6))\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "# This builds the model for the first time:\n",
    "model.fit(x, y, batch_size=10, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x  = swin(x)\n",
    "#print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swin.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.constant(-100, dtype= 'float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "from keras.layers import Conv3D , LayerNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 96\n",
    "patch_size = (2,7,7)\n",
    "\n",
    "projection = tf.keras.Sequential(\n",
    "[\n",
    "    Conv3D(\n",
    "        embed_dim ,kernel_size = patch_size , strides= patch_size , padding=\"valid\", name= \"conv_projection\"\n",
    "    )\n",
    "],\n",
    "name = \"projection\"\n",
    ")   # data_format= \"channels_first\"\n",
    "\n",
    "projection.add(LayerNormalization(epsilon=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pt = torch.rand( (1,3, 8,224,224)) * 255\n",
    "x_np = x_pt.numpy()\n",
    "x_tf = tf.convert_to_tensor(x_np)\n",
    "x_tf = tf.transpose(x_tf, perm=(0,2,3,4,1))\n",
    "x_tf = tf.cast(x_tf, dtype= tf.float32)\n",
    "\n",
    "x_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "18e9ffd7bdaebed3141c5f1e6e3ffefff8dc763f7fe0a2903683245d14d535a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
