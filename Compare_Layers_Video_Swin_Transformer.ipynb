{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VSHDVCPoT6S1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from VideoSwinTransformer import *\n",
        "import os\n",
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88Jni1OfBmgZ",
        "outputId": "6c37df68-467f-4354-fbce-d82a0ec27a94"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x1d24b67fdf0>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.set_grad_enabled(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WQP-DjEmTzcY"
      },
      "outputs": [],
      "source": [
        "def get_x():\n",
        "    x_pt = torch.rand((1,3,8,224,224)) * 255\n",
        "    x_np = x_pt.numpy()\n",
        "    x_tf = tf.convert_to_tensor(x_np)\n",
        "\n",
        "    return x_tf, x_pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wp-MvywEL5XT"
      },
      "outputs": [],
      "source": [
        "x_tf, x_pt = get_x()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgYW2rAqTzcZ"
      },
      "source": [
        "## Convert Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUYoVlOfTzcc"
      },
      "source": [
        "Initiate model and Load PyTorch Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6aRD7ZyUcWl",
        "outputId": "d62ec70a-4a8d-4c8d-cd96-45788e967867"
      },
      "outputs": [],
      "source": [
        "cfg_method = model_configs.MODEL_MAP[\"swin_tiny_patch244_window877_kinetics400_1k\"]\n",
        "cfg = cfg_method()\n",
        "\n",
        "name = cfg[\"name\"]\n",
        "link = cfg['link']\n",
        "del cfg[\"name\"]\n",
        "del cfg['link']\n",
        "del cfg[\"drop_path_rate\"]\n",
        "# download_weight_command = f\"wget {link} -O {name}.pth\"\n",
        "# os.system(download_weight_command)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "emFflCpfTzcd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python\\Python396\\lib\\site-packages\\torch\\functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:2895.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_window_size parameters (4, 56, 56) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "compute mask parameters (4, 56, 56, (4, 7, 7), (0, 3, 3))\n",
            "attn_mask (64, 196, 196)\n",
            "get_window_size parameters (4, 56, 56) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "compute mask parameters (4, 56, 56, (4, 7, 7), (0, 3, 3))\n",
            "attn_mask (64, 196, 196)\n",
            "get_window_size parameters (4, 28, 28) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "compute mask parameters (4, 28, 28, (4, 7, 7), (0, 3, 3))\n",
            "attn_mask (16, 196, 196)\n",
            "get_window_size parameters (4, 28, 28) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "compute mask parameters (4, 28, 28, (4, 7, 7), (0, 3, 3))\n",
            "attn_mask (16, 196, 196)\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "compute mask parameters (4, 14, 14, (4, 7, 7), (0, 3, 3))\n",
            "attn_mask (4, 196, 196)\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "compute mask parameters (4, 14, 14, (4, 7, 7), (0, 3, 3))\n",
            "attn_mask (4, 196, 196)\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "compute mask parameters (4, 14, 14, (4, 7, 7), (0, 3, 3))\n",
            "attn_mask (4, 196, 196)\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "compute mask parameters (4, 14, 14, (4, 7, 7), (0, 3, 3))\n",
            "attn_mask (4, 196, 196)\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "compute mask parameters (4, 14, 14, (4, 7, 7), (0, 3, 3))\n",
            "attn_mask (4, 196, 196)\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "compute mask parameters (4, 14, 14, (4, 7, 7), (0, 3, 3))\n",
            "attn_mask (4, 196, 196)\n",
            "get_window_size parameters (4, 7, 7) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "compute mask parameters (4, 7, 7, (4, 7, 7), (0, 0, 0))\n",
            "attn_mask (1, 196, 196)\n",
            "get_window_size parameters (4, 7, 7) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "compute mask parameters (4, 7, 7, (4, 7, 7), (0, 0, 0))\n",
            "attn_mask (1, 196, 196)\n",
            "\n",
            "get_window_size parameters (4, 56, 56) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "attn_mask <built-in method size of Tensor object at 0x000001D266D99DB0>\n",
            "compute mask parameters (4, 56, 56, (4, 7, 7), (0, 3, 3))\n",
            "get_window_size parameters (4, 56, 56) (8, 7, 7) (0, 0, 0)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "get_window_size parameters (4, 56, 56) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "\n",
            "get_window_size parameters (4, 28, 28) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "attn_mask <built-in method size of Tensor object at 0x000001D266DA3090>\n",
            "compute mask parameters (4, 28, 28, (4, 7, 7), (0, 3, 3))\n",
            "get_window_size parameters (4, 28, 28) (8, 7, 7) (0, 0, 0)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "get_window_size parameters (4, 28, 28) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "attn_mask <built-in method size of Tensor object at 0x000001D266DA32C0>\n",
            "compute mask parameters (4, 14, 14, (4, 7, 7), (0, 3, 3))\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (0, 0, 0)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (0, 0, 0)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (0, 0, 0)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "\n",
            "get_window_size parameters (4, 7, 7) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "attn_mask <built-in method size of Tensor object at 0x000001D266DA3180>\n",
            "compute mask parameters (4, 7, 7, (4, 7, 7), (0, 0, 0))\n",
            "get_window_size parameters (4, 7, 7) (8, 7, 7) (0, 0, 0)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "get_window_size parameters (4, 7, 7) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "-------\n",
            "\n",
            "self.compute_mask_info {'shape_of_input': (2, 96, 2, 56, 56), 'window_size': (8, 7, 7), 'shift_size': (4, 3, 3)}\n",
            "get_window_size parameters (4, 56, 56) (8, 7, 7) (0, 0, 0)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "get_window_size parameters (4, 56, 56) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "\n",
            "self.compute_mask_info {'shape_of_input': (2, 192, 2, 28, 28), 'window_size': (8, 7, 7), 'shift_size': (4, 3, 3)}\n",
            "get_window_size parameters (4, 28, 28) (8, 7, 7) (0, 0, 0)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "get_window_size parameters (4, 28, 28) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "\n",
            "self.compute_mask_info {'shape_of_input': (2, 384, 2, 14, 14), 'window_size': (8, 7, 7), 'shift_size': (4, 3, 3)}\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (0, 0, 0)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (0, 0, 0)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (0, 0, 0)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "\n",
            "self.compute_mask_info {'shape_of_input': (2, 768, 2, 7, 7), 'window_size': (8, 7, 7), 'shift_size': (4, 3, 3)}\n",
            "get_window_size parameters (4, 7, 7) (8, 7, 7) (0, 0, 0)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "get_window_size parameters (4, 7, 7) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 0, 0)\n"
          ]
        }
      ],
      "source": [
        "pt_model = SwinTransformer3D_pt(**cfg,drop_rate=0.4, drop_path_rate=0., isTest= True)\n",
        "tf_model = SwinTransformer3D(**cfg,drop_rate=0.4, drop_path_rate=0., isTest= True)\n",
        "x_tf, x_pt = get_x()\n",
        "\n",
        "\n",
        "\n",
        "basic_pt, z= pt_model(x_pt)\n",
        "print(\"-------\")\n",
        "\n",
        "basic_tf, y = tf_model(x_tf)\n",
        "checkpoint = torch.load(f'{name}.pth')\n",
        "\n",
        "\n",
        "\n",
        "new_state_dict = OrderedDict()\n",
        "for k, v in checkpoint['state_dict'].items():\n",
        "    if 'backbone' in k:\n",
        "        name = k[9:]\n",
        "        new_state_dict[name] = v \n",
        "\n",
        "pt_model.load_state_dict(new_state_dict) \n",
        "pt_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfFZyWSaTzcf"
      },
      "source": [
        "Convert Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "02w5N8_mTzcg"
      },
      "outputs": [],
      "source": [
        "def conv_transpose(w):\n",
        "    return w.transpose(2,3,4,1, 0)\n",
        "    \n",
        "\n",
        "def modify_tf_block( tf_component, pt_weight,  pt_bias = None, is_attn=False):\n",
        "    in_shape = pt_weight.shape\n",
        "\n",
        "    if isinstance(tf_component, tf.keras.layers.Conv3D) :\n",
        "      pt_weight = conv_transpose(pt_weight)\n",
        "\n",
        "    if isinstance(tf_component, tf.keras.layers.Dense) and not is_attn:\n",
        "      pt_weight =pt_weight.transpose()\n",
        "\n",
        "    if isinstance(tf_component, (tf.keras.layers.Dense, tf.keras.layers.Conv3D)):\n",
        "        tf_component.kernel.assign(tf.Variable(pt_weight))\n",
        "\n",
        "        if pt_bias is not None:\n",
        "            tf_component.bias.assign(tf.Variable(pt_bias))\n",
        "\n",
        "    elif isinstance(tf_component, tf.keras.layers.LayerNormalization):\n",
        "\n",
        "        tf_component.gamma.assign(tf.Variable(pt_weight))\n",
        "\n",
        "        tf_component.beta.assign(tf.Variable(pt_bias))\n",
        "\n",
        "    elif isinstance(tf_component, (tf.Variable)):\n",
        "        tf_component.assign(tf.Variable(pt_weight))\n",
        "\n",
        "    else:\n",
        "        return tf.convert_to_tensor(pt_weight)\n",
        "        \n",
        "        \n",
        "\n",
        "    return tf_component\n",
        "\n",
        "\n",
        "def modify_swin_blocks(np_state_dict, pt_weights_prefix, tf_block):\n",
        "\n",
        "  for layer in tf_block:\n",
        "    if isinstance(layer, PatchMerging):\n",
        "      patch_merging_idx = f\"{pt_weights_prefix}.downsample\"\n",
        "\n",
        "      layer.reduction = modify_tf_block( layer.reduction,\n",
        "                          np_state_dict[f\"{patch_merging_idx}.reduction.weight\"])\n",
        "      layer.norm = modify_tf_block( layer.norm,\n",
        "                        np_state_dict[f\"{patch_merging_idx}.norm.weight\"],\n",
        "                        np_state_dict[f\"{patch_merging_idx}.norm.bias\"]\n",
        "                        )\n",
        "      \n",
        "  # Swin Layers\n",
        "  common_prefix = f\"{pt_weights_prefix}.blocks\"\n",
        "  block_idx = 0\n",
        "\n",
        "  for outer_layer in tf_block:\n",
        "\n",
        "      layernorm_idx = 1\n",
        "      mlp_layer_idx = 1\n",
        "\n",
        "      if isinstance(outer_layer, SwinTransformerBlock3D):\n",
        "          for inner_layer in outer_layer.layers:\n",
        "        \n",
        "              # Layer norm.\n",
        "              if isinstance(inner_layer, tf.keras.layers.LayerNormalization):\n",
        "                  layer_norm_prefix = (\n",
        "                      f\"{common_prefix}.{block_idx}.norm{layernorm_idx}\"\n",
        "                  )\n",
        "                  inner_layer.gamma.assign(\n",
        "                      tf.Variable(\n",
        "                          np_state_dict[f\"{layer_norm_prefix}.weight\"]\n",
        "                      )\n",
        "                  )\n",
        "\n",
        "\n",
        "\n",
        "                  inner_layer.beta.assign(\n",
        "                      tf.Variable(np_state_dict[f\"{layer_norm_prefix}.bias\"])\n",
        "                  )\n",
        "\n",
        "                  layernorm_idx += 1\n",
        "\n",
        "              # Window attention.\n",
        "              elif isinstance(inner_layer, WindowAttention3D):\n",
        "                  attn_prefix = f\"{common_prefix}.{block_idx}.attn\"\n",
        "\n",
        "                  # Relative position.\n",
        "                  inner_layer.relative_position_bias_table = (\n",
        "                      modify_tf_block(\n",
        "                          inner_layer.relative_position_bias_table,\n",
        "                          np_state_dict[\n",
        "                              f\"{attn_prefix}.relative_position_bias_table\"\n",
        "                          ] \n",
        "                      )\n",
        "                  )\n",
        "                  inner_layer.relative_position_index = (\n",
        "                      modify_tf_block(\n",
        "                          inner_layer.relative_position_index,\n",
        "                          np_state_dict[\n",
        "                              f\"{attn_prefix}.relative_position_index\"\n",
        "                          ]\n",
        "                      )\n",
        "                  )\n",
        "\n",
        "                  # QKV.\n",
        "                  inner_layer.qkv = modify_tf_block(\n",
        "                      inner_layer.qkv,\n",
        "                      np_state_dict[f\"{attn_prefix}.qkv.weight\"],\n",
        "                      np_state_dict[f\"{attn_prefix}.qkv.bias\"]\n",
        "                  )\n",
        "\n",
        "                  # Projection.\n",
        "                  inner_layer.proj = modify_tf_block(\n",
        "                      inner_layer.proj,\n",
        "                      np_state_dict[f\"{attn_prefix}.proj.weight\"],\n",
        "                      np_state_dict[f\"{attn_prefix}.proj.bias\"]\n",
        "                  )\n",
        "\n",
        "              # MLP.\n",
        "              elif isinstance(inner_layer, tf.keras.Model):\n",
        "                  mlp_prefix = f\"{common_prefix}.{block_idx}.mlp\"\n",
        "                  for mlp_layer in inner_layer.layers:\n",
        "                      if isinstance(mlp_layer, tf.keras.layers.Dense):\n",
        "                          mlp_layer = modify_tf_block(\n",
        "                              mlp_layer,\n",
        "                              np_state_dict[\n",
        "                                  f\"{mlp_prefix}.fc{mlp_layer_idx}.weight\"\n",
        "                              ],\n",
        "                              np_state_dict[\n",
        "                                  f\"{mlp_prefix}.fc{mlp_layer_idx}.bias\"\n",
        "                              ]\n",
        "                          )\n",
        "                          mlp_layer_idx += 1\n",
        "\n",
        "          block_idx += 1\n",
        "  return tf_block\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGfcatguTzck"
      },
      "source": [
        "### convert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pZAMhaFGTzcl"
      },
      "outputs": [],
      "source": [
        "np_state_dict = pt_model.state_dict()\n",
        "np_state_dict = {k: np_state_dict[k].numpy() for k in np_state_dict}\n",
        "\n",
        "tf_model.projection.layers[0] = modify_tf_block(tf_model.projection.layers[0]\n",
        "        ,\n",
        "        np_state_dict[\"patch_embed.proj.weight\"],\n",
        "        np_state_dict[\"patch_embed.proj.bias\"])\n",
        "\n",
        "tf_model.projection.layers[1] = modify_tf_block(\n",
        "    tf_model.projection.layers[1],\n",
        "    np_state_dict[\"patch_embed.norm.weight\"],\n",
        "    np_state_dict[\"patch_embed.norm.bias\"])\n",
        "\n",
        "\n",
        "layer_normalization_idx = -1\n",
        "\n",
        "tf_model.layers[layer_normalization_idx] = modify_tf_block(\n",
        "    tf_model.layers[layer_normalization_idx] ,\n",
        "    np_state_dict[\"norm.weight\"],\n",
        "    np_state_dict[\"norm.bias\"]\n",
        "    )\n",
        "\n",
        "# swin layers\n",
        "for i in range(2, len(tf_model.layers) - 1):\n",
        "    _ = modify_swin_blocks(np_state_dict,\n",
        "                        f\"layers.{i-2}\",\n",
        "                        tf_model.layers[i].layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KBvHCrG9oxyu"
      },
      "outputs": [],
      "source": [
        "x_tf, x_pt = get_x()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sB9un1CATzcm",
        "outputId": "6d2e86a7-32bb-4601-aaa1-9e6547eeebdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "self.compute_mask_info {'shape_of_input': (2, 96, 2, 56, 56), 'window_size': (8, 7, 7), 'shift_size': (4, 3, 3)}\n",
            "get_window_size parameters (4, 56, 56) (8, 7, 7) (0, 0, 0)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "get_window_size parameters (4, 56, 56) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "\n",
            "self.compute_mask_info {'shape_of_input': (2, 192, 2, 28, 28), 'window_size': (8, 7, 7), 'shift_size': (4, 3, 3)}\n",
            "get_window_size parameters (4, 28, 28) (8, 7, 7) (0, 0, 0)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "get_window_size parameters (4, 28, 28) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "\n",
            "self.compute_mask_info {'shape_of_input': (2, 384, 2, 14, 14), 'window_size': (8, 7, 7), 'shift_size': (4, 3, 3)}\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (0, 0, 0)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (0, 0, 0)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (0, 0, 0)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "\n",
            "self.compute_mask_info {'shape_of_input': (2, 768, 2, 7, 7), 'window_size': (8, 7, 7), 'shift_size': (4, 3, 3)}\n",
            "get_window_size parameters (4, 7, 7) (8, 7, 7) (0, 0, 0)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "get_window_size parameters (4, 7, 7) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "------\n",
            "\n",
            "get_window_size parameters (4, 56, 56) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "attn_mask <built-in method size of Tensor object at 0x000001D266D99DB0>\n",
            "compute mask parameters (4, 56, 56, (4, 7, 7), (0, 3, 3))\n",
            "get_window_size parameters (4, 56, 56) (8, 7, 7) (0, 0, 0)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "get_window_size parameters (4, 56, 56) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "\n",
            "get_window_size parameters (4, 28, 28) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "attn_mask <built-in method size of Tensor object at 0x000001D266DA3090>\n",
            "compute mask parameters (4, 28, 28, (4, 7, 7), (0, 3, 3))\n",
            "get_window_size parameters (4, 28, 28) (8, 7, 7) (0, 0, 0)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "get_window_size parameters (4, 28, 28) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "attn_mask <built-in method size of Tensor object at 0x000001D266DA32C0>\n",
            "compute mask parameters (4, 14, 14, (4, 7, 7), (0, 3, 3))\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (0, 0, 0)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (0, 0, 0)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (0, 0, 0)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "get_window_size parameters (4, 14, 14) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 3, 3)\n",
            "\n",
            "get_window_size parameters (4, 7, 7) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "attn_mask <built-in method size of Tensor object at 0x000001D266DA3180>\n",
            "compute mask parameters (4, 7, 7, (4, 7, 7), (0, 0, 0))\n",
            "get_window_size parameters (4, 7, 7) (8, 7, 7) (0, 0, 0)\n",
            "(4, 7, 7) (0, 0, 0)\n",
            "get_window_size parameters (4, 7, 7) (8, 7, 7) (4, 3, 3)\n",
            "(4, 7, 7) (0, 0, 0)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(TensorShape([1, 768, 4, 7, 7]), torch.Size([1, 768, 4, 7, 7]))"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "layers_output_tf, y = tf_model(x_tf)\n",
        "print(\"------\")\n",
        "layers_output_pt, z= pt_model(x_pt)\n",
        "\n",
        "y.shape, z.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "jus_cUshN-f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------- PatchEmbed -------------\n",
            " TF:  [[[[[-1.52765    -1.8028401  -1.3727719  -1.512826    0.75105476\n",
            "     -0.6499636  -0.21560025 -1.6660732  -0.58690006 -0.07118201]]]]] \n",
            " PT:  [[[[[-1.52765    -1.8028399  -1.372772   -1.5128261   0.751055\n",
            "     -0.64996356 -0.21560037 -1.6660732  -0.58690006 -0.07118207]]]]] \n",
            "\n",
            "-------------- drop_out -------------\n",
            " TF:  [[[[[-1.52765    -1.8028401  -1.3727719  -1.512826    0.75105476\n",
            "     -0.6499636  -0.21560025 -1.6660732  -0.58690006 -0.07118201]]]]] \n",
            " PT:  [[[[[-1.52765    -1.8028399  -1.372772   -1.5128261   0.751055\n",
            "     -0.64996356 -0.21560037 -1.6660732  -0.58690006 -0.07118207]]]]] \n",
            "\n",
            "-------------- basic layer1 -------------\n",
            " TF:  [[[[[ 0.6290703   0.48750663 -0.14188242  0.41177928  0.28953955\n",
            "      0.8019364   0.7212719   0.549551    0.40311396  0.27723506]]]]] \n",
            " PT:  [[[[[ 0.6290704   0.48750666 -0.1418823   0.41177952  0.2895401\n",
            "      0.8019364   0.7212718   0.5495511   0.40311405  0.277235  ]]]]] \n",
            "\n",
            "-------------- basic layer2 -------------\n",
            " TF:  [[[[[-2.4347944 -2.2291362 -2.1214926 -2.1661928 -1.5803608 -1.9031911\n",
            "     -1.7683102 -2.1788588 -2.294505  -2.2266617]]]]] \n",
            " PT:  [[[[[-2.4347954 -2.2291362 -2.1214924 -2.166193  -1.5803607 -1.9031907\n",
            "     -1.7683102 -2.1788588 -2.2945051 -2.2266617]]]]] \n",
            "\n",
            "-------------- basic layer3 -------------\n",
            " TF:  [[[[[0.25213856 0.15416926 0.30459815 0.2227095  0.15994994 0.293455\n",
            "     0.36172807]]]]] \n",
            " PT:  [[[[[0.25213835 0.15416914 0.30459797 0.22270948 0.15994975 0.29345524\n",
            "     0.3617281 ]]]]] \n",
            "\n",
            "-------------- basic layer4 -------------\n",
            " TF:  [[[[[ 1.0130537   0.65057427  1.0032165   0.96412134  0.81769437\n",
            "     -0.56854486 -0.52120453]]]]] \n",
            " PT:  [[[[[ 0.99976957  0.6757068   1.0064611   0.96450347  0.79991484\n",
            "     -0.57283354 -0.53025645]]]]] \n",
            "\n",
            "-------------- Final Output -------------\n",
            " TF:  [[[[[ 0.26910302  0.2987706   0.3199229   0.2634662   0.27829856\n",
            "     -0.08607621 -0.07679231]]]]] \n",
            " PT:  [[[[[ 0.2675702   0.31331113  0.3174837   0.2645165   0.26967835\n",
            "     -0.08652331 -0.07905345]]]]] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Eplore the output of all layers\n",
        "for layer in layers_output_pt:\n",
        "    print(\"--------------\",layer, \"-------------\\n TF: \", layers_output_tf[layer].numpy()[:1,:1,:1,:1,:10], \"\\n PT: \", layers_output_pt[layer].detach().numpy()[:1,:1,:1,:1,:10], \"\\n\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing PatchEmbed\n",
            "Testing drop_out\n",
            "Testing basic layer1\n",
            "Testing basic layer2\n",
            "Testing basic layer3\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "\nNot equal to tolerance rtol=0.0001, atol=0.0001\n\nMismatched elements: 1 / 150528 (0.000664%)\nMax absolute difference: 0.00127792\nMax relative difference: 0.3161512\n x: array([[[[[ 2.521386e-01,  1.541693e-01,  3.045982e-01, ...,\n            1.599499e-01,  2.934550e-01,  3.617281e-01],\n          [ 1.779881e-01,  1.574906e-01,  3.287339e-01, ...,...\n y: array([[[[[ 2.521383e-01,  1.541691e-01,  3.045980e-01, ...,\n            1.599497e-01,  2.934552e-01,  3.617281e-01],\n          [ 1.779877e-01,  1.574907e-01,  3.287338e-01, ...,...",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8176/3034020284.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlayers_output_pt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Testing\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_allclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers_output_tf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers_output_pt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[1;32mc:\\Python\\Python396\\lib\\site-packages\\numpy\\testing\\_private\\utils.py\u001b[0m in \u001b[0;36massert_array_compare\u001b[1;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001b[0m\n\u001b[0;32m    844\u001b[0m                                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m                                 names=('x', 'y'), precision=precision)\n\u001b[1;32m--> 846\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    847\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mAssertionError\u001b[0m: \nNot equal to tolerance rtol=0.0001, atol=0.0001\n\nMismatched elements: 1 / 150528 (0.000664%)\nMax absolute difference: 0.00127792\nMax relative difference: 0.3161512\n x: array([[[[[ 2.521386e-01,  1.541693e-01,  3.045982e-01, ...,\n            1.599499e-01,  2.934550e-01,  3.617281e-01],\n          [ 1.779881e-01,  1.574906e-01,  3.287339e-01, ...,...\n y: array([[[[[ 2.521383e-01,  1.541691e-01,  3.045980e-01, ...,\n            1.599497e-01,  2.934552e-01,  3.617281e-01],\n          [ 1.779877e-01,  1.574907e-01,  3.287338e-01, ...,..."
          ]
        }
      ],
      "source": [
        "# compare layers' output. It asserts  at basic layer1\n",
        "for layer in layers_output_pt:\n",
        "    print(\"Testing\", layer)\n",
        "    np.testing.assert_allclose(layers_output_tf[layer].numpy(), layers_output_pt[layer].detach().numpy(), 1e-4, 1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "\nNot equal to tolerance rtol=0.0001, atol=0.0001\n\nMismatched elements: 149169 / 150528 (99.1%)\nMax absolute difference: 3.9597473\nMax relative difference: 1190.8073\n x: array([[[[[ 1.013054e+00,  6.505743e-01,  1.003217e+00, ...,\n            8.176944e-01, -5.685449e-01, -5.212045e-01],\n          [ 2.717099e-01,  2.975306e-01, -3.709410e-01, ...,...\n y: array([[[[[ 9.997696e-01,  6.757068e-01,  1.006461e+00, ...,\n            7.999148e-01, -5.728335e-01, -5.302565e-01],\n          [ 2.555471e-01,  2.936881e-01, -3.771242e-01, ...,...",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8176/2480737440.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_allclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers_output_tf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"basic layer4\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers_output_pt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"basic layer4\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[1;32mc:\\Python\\Python396\\lib\\site-packages\\numpy\\testing\\_private\\utils.py\u001b[0m in \u001b[0;36massert_array_compare\u001b[1;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001b[0m\n\u001b[0;32m    844\u001b[0m                                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m                                 names=('x', 'y'), precision=precision)\n\u001b[1;32m--> 846\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    847\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mAssertionError\u001b[0m: \nNot equal to tolerance rtol=0.0001, atol=0.0001\n\nMismatched elements: 149169 / 150528 (99.1%)\nMax absolute difference: 3.9597473\nMax relative difference: 1190.8073\n x: array([[[[[ 1.013054e+00,  6.505743e-01,  1.003217e+00, ...,\n            8.176944e-01, -5.685449e-01, -5.212045e-01],\n          [ 2.717099e-01,  2.975306e-01, -3.709410e-01, ...,...\n y: array([[[[[ 9.997696e-01,  6.757068e-01,  1.006461e+00, ...,\n            7.999148e-01, -5.728335e-01, -5.302565e-01],\n          [ 2.555471e-01,  2.936881e-01, -3.771242e-01, ...,..."
          ]
        }
      ],
      "source": [
        "e = 1e-4\n",
        "np.testing.assert_allclose(layers_output_tf[\"basic layer4\"].numpy(), layers_output_pt[\"basic layer4\"].detach().numpy(), e, e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "lTsJhqo39sOx"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "\nNot equal to tolerance rtol=0.01, atol=0.01\n\nMismatched elements: 17569 / 150528 (11.7%)\nMax absolute difference: 0.24690652\nMax relative difference: 1237.8418\n x: array([[[[[ 2.691030e-01,  2.987706e-01,  3.199229e-01, ...,\n            2.782986e-01, -8.607621e-02, -7.679231e-02],\n          [ 7.984339e-02,  1.169502e-01, -6.092245e-02, ...,...\n y: array([[[[[ 2.675702e-01,  3.133111e-01,  3.174837e-01, ...,\n            2.696784e-01, -8.652331e-02, -7.905345e-02],\n          [ 7.668141e-02,  1.140875e-01, -6.171951e-02, ...,...",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8176/2720901724.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# comparing the outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_allclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[1;32mc:\\Python\\Python396\\lib\\site-packages\\numpy\\testing\\_private\\utils.py\u001b[0m in \u001b[0;36massert_array_compare\u001b[1;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001b[0m\n\u001b[0;32m    844\u001b[0m                                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m                                 names=('x', 'y'), precision=precision)\n\u001b[1;32m--> 846\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    847\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mAssertionError\u001b[0m: \nNot equal to tolerance rtol=0.01, atol=0.01\n\nMismatched elements: 17569 / 150528 (11.7%)\nMax absolute difference: 0.24690652\nMax relative difference: 1237.8418\n x: array([[[[[ 2.691030e-01,  2.987706e-01,  3.199229e-01, ...,\n            2.782986e-01, -8.607621e-02, -7.679231e-02],\n          [ 7.984339e-02,  1.169502e-01, -6.092245e-02, ...,...\n y: array([[[[[ 2.675702e-01,  3.133111e-01,  3.174837e-01, ...,\n            2.696784e-01, -8.652331e-02, -7.905345e-02],\n          [ 7.668141e-02,  1.140875e-01, -6.171951e-02, ...,..."
          ]
        }
      ],
      "source": [
        "# comparing the outputs\n",
        "np.testing.assert_allclose(y.numpy(), z.detach().numpy(), 1e-2, 1e-2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "ddWr78VQ-oGa",
        "outputId": "f18fdf52-a55a-45c4-e2e8-ef183b13bef7"
      },
      "outputs": [],
      "source": [
        "# compare layers' output. It asserts  at basic layer1\n",
        "for layer in layers_output_pt:\n",
        "    print(\"Testing\", layer)\n",
        "    np.testing.assert_allclose(layers_output_tf[layer].numpy(), layers_output_pt[layer].detach().numpy(), 1e-4, 1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDYo4pqMTzcn"
      },
      "source": [
        "### PT basic layer outputs comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qry9YZwqTzcn"
      },
      "outputs": [],
      "source": [
        "attempts = 10\n",
        "# x_pt = torch.rand((1,3,8,224,224))\n",
        "\n",
        "outputs = []\n",
        "\n",
        "for i in range(attempts):\n",
        "    layer_out , result = pt_model(x_pt)\n",
        "    outputs.append(layer_out)\n",
        "\n",
        "i = 0\n",
        "for layer in outputs[0]:\n",
        "    print(\"--------------\", layer, \"---------------\")\n",
        "    for idx, layer_out in enumerate(outputs) :\n",
        "        print(f\"attempt {idx} : \",layer_out[layer].detach().numpy()[:1,:1,:1,:1,:10], \"\\n\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XThvAWb2AOT1"
      },
      "source": [
        "### All close testing\n",
        "\n",
        "Compare the first attempt with another attempt. Enter the attempt value in the following cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58uXm8HnDnWy"
      },
      "outputs": [],
      "source": [
        "attempt_no = 2        # Change the attempt_no value to compare the first attempt with another attempt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VI7BMA75ES86"
      },
      "outputs": [],
      "source": [
        "# PatchEmbed Layer\n",
        "output1 = outputs[0][\"PatchEmbed\"]\n",
        "output2 =  outputs[attempt_no][\"PatchEmbed\"]\n",
        "\n",
        "np.testing.assert_allclose(output1.detach().numpy(), output2.detach().numpy(), 1e-4, 1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozPQ2dK5DOBy"
      },
      "outputs": [],
      "source": [
        "# Basic Layer1\n",
        "output1 = outputs[0][\"basic layer1\"]\n",
        "output2 =  outputs[attempt_no][\"basic layer1\"]\n",
        "\n",
        "np.testing.assert_allclose(output1.detach().numpy(), output2.detach().numpy(), 1e-4, 1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rF5wzPmyoZKz"
      },
      "outputs": [],
      "source": [
        "# Basic Layer2\n",
        "\n",
        "output1 = outputs[0][\"basic layer2\"]\n",
        "output2 =  outputs[attempt_no][\"basic layer2\"]\n",
        "\n",
        "np.testing.assert_allclose(output1.detach().numpy(), output2.detach().numpy(), 1e-4, 1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38msQO2V_Ymo"
      },
      "outputs": [],
      "source": [
        "# Basic Layer3\n",
        "\n",
        "output1 = outputs[0][\"basic layer3\"]\n",
        "output2 =  outputs[attempt_no][\"basic layer3\"]\n",
        "\n",
        "np.testing.assert_allclose(output1.detach().numpy(), output2.detach().numpy(), 1e-4, 1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvQENbPDEgv7"
      },
      "outputs": [],
      "source": [
        "# Basic Layer4\n",
        "\n",
        "output1 = outputs[0][\"basic layer4\"]\n",
        "output2 =  outputs[attempt_no][\"basic layer4\"]\n",
        "\n",
        "np.testing.assert_allclose(output1.detach().numpy(), output2.detach().numpy(), 1e-4, 1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdczhGavDSyg"
      },
      "outputs": [],
      "source": [
        "# Final Output\n",
        "\n",
        "output1 = outputs[0][\"Final Output\"]\n",
        "output2 =  outputs[attempt_no][\"Final Output\"]\n",
        "\n",
        "np.testing.assert_allclose(output1.detach().numpy(), output2.detach().numpy(), 1e-4, 1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6LqDD8hDVAn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Compare Layers Video Swin Transformer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "5ed124f2b279c186db57d0ede455df2b109954ad4fa1a5a2c59adb747c894aa2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
