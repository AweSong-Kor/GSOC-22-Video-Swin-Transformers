{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x134ef6fd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from VideoSwinTransformer import *\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x(shape= (4,3,32,224,224)):\n",
    "    x_pt = torch.rand(shape) * 255\n",
    "    x_np = x_pt.numpy()\n",
    "    x_tf = tf.convert_to_tensor(x_np)\n",
    "\n",
    "    return x_tf, x_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-06 12:49:23.893876: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-09-06 12:49:23.893999: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shape_of_input = [5,3,32,224,224]\n",
    "x_tf, x_pt = get_x(shape_of_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_method = model_configs.MODEL_MAP[\"swin_tiny_patch244_window877_kinetics400_1k\"]\n",
    "cfg = cfg_method()\n",
    "\n",
    "name = cfg[\"name\"]\n",
    "link = cfg['link']\n",
    "del cfg[\"name\"]\n",
    "del cfg['link']\n",
    "del cfg[\"drop_path_rate\"]\n",
    "# download_weight_command = f\"wget {link} -O {name}.pth\"\n",
    "# os.system(download_weight_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape_of_input [4, 3, 32, 224, 224]\n",
      "pt\n",
      "++++++++\n",
      " tf\n",
      "shape_of_input:  [4, 3, 32, 224, 224]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammadshoaib/Codes/tensorflow-test/env/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " pt\n",
      "drop path shape (4, 1, 1, 1, 1)\n",
      "drop path shape (4, 1, 1, 1, 1)\n",
      "drop path shape (4, 1, 1, 1, 1)\n",
      "drop path shape (4, 1, 1, 1, 1)\n",
      "drop path shape (4, 1, 1, 1, 1)\n",
      "drop path shape (4, 1, 1, 1, 1)\n",
      "drop path shape (4, 1, 1, 1, 1)\n",
      "drop path shape (4, 1, 1, 1, 1)\n",
      "drop path shape (4, 1, 1, 1, 1)\n",
      "drop path shape (4, 1, 1, 1, 1)\n",
      "drop path shape (4, 1, 1, 1, 1)\n",
      "drop path shape (4, 1, 1, 1, 1)\n",
      "drop path shape (4, 1, 1, 1, 1)\n",
      "drop path shape (4, 1, 1, 1, 1)\n",
      "drop path shape (4, 1, 1, 1, 1)\n",
      "drop path shape (4, 1, 1, 1, 1)\n",
      "drop path shape (4, 1, 1, 1, 1)\n",
      "drop path shape (4, 1, 1, 1, 1)\n",
      "drop path shape (4, 1, 1, 1, 1)\n",
      "drop path shape (4, 1, 1, 1, 1)\n",
      "drop path shape (4, 1, 1, 1, 1)\n",
      "drop path shape (4, 1, 1, 1, 1)\n",
      "------- \n",
      " tf\n",
      "drop path shape (4, 16, 56, 56, 96)\n",
      "drop path shape (4, 16, 56, 56, 96)\n",
      "drop path shape (4, 16, 28, 28, 192)\n",
      "drop path shape (4, 16, 28, 28, 192)\n",
      "drop path shape (4, 16, 28, 28, 192)\n",
      "drop path shape (4, 16, 28, 28, 192)\n",
      "drop path shape (4, 16, 14, 14, 384)\n",
      "drop path shape (4, 16, 14, 14, 384)\n",
      "drop path shape (4, 16, 14, 14, 384)\n",
      "drop path shape (4, 16, 14, 14, 384)\n",
      "drop path shape (4, 16, 14, 14, 384)\n",
      "drop path shape (4, 16, 14, 14, 384)\n",
      "drop path shape (4, 16, 14, 14, 384)\n",
      "drop path shape (4, 16, 14, 14, 384)\n",
      "drop path shape (4, 16, 14, 14, 384)\n",
      "drop path shape (4, 16, 14, 14, 384)\n",
      "drop path shape (4, 16, 14, 14, 384)\n",
      "drop path shape (4, 16, 14, 14, 384)\n",
      "drop path shape (4, 16, 7, 7, 768)\n",
      "drop path shape (4, 16, 7, 7, 768)\n",
      "drop path shape (4, 16, 7, 7, 768)\n",
      "drop path shape (4, 16, 7, 7, 768)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape_of_input\", shape_of_input)\n",
    "print(\"pt\")\n",
    "pt_model = SwinTransformer3D_pt(**cfg)\n",
    "print(\"++++++++\\n tf\")\n",
    "tf_model = SwinTransformer3D(**cfg,shape_of_input=shape_of_input)\n",
    "x_tf, x_pt = get_x()\n",
    "\n",
    "\n",
    "print(\"\\n\\n\\n pt\")\n",
    "z= pt_model(x_pt)\n",
    "print(\"------- \\n tf\")\n",
    "# x  = tf.keras.layers.Input(tensor=x_tf)\n",
    "\n",
    "y = tf_model(x_tf, training= True)\n",
    "\n",
    "# print(\"\\n loading checkpoint\")\n",
    "# checkpoint = torch.load(f'{name}.pth')\n",
    "# new_state_dict = OrderedDict()\n",
    "# for k, v in checkpoint['state_dict'].items():\n",
    "#     if 'backbone' in k:\n",
    "#         name = k[9:]\n",
    "#         new_state_dict[name] = v \n",
    "\n",
    "# pt_model.load_state_dict(new_state_dict) \n",
    "# pt_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/mohammadshoaib/Codes/tensorflow-test/GSOC-22-Video-Swin-Transformers/check_saved_model.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mohammadshoaib/Codes/tensorflow-test/GSOC-22-Video-Swin-Transformers/check_saved_model.ipynb#ch0000017?line=0'>1</a>\u001b[0m a\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_transpose(w):\n",
    "    return w.transpose(2,3,4,1, 0)\n",
    "    \n",
    "\n",
    "def modify_tf_block( tf_component, pt_weight,  pt_bias = None, is_attn=False):\n",
    "    in_shape = pt_weight.shape\n",
    "\n",
    "    if isinstance(tf_component, tf.keras.layers.Conv3D) :\n",
    "      pt_weight = conv_transpose(pt_weight)\n",
    "\n",
    "    if isinstance(tf_component, tf.keras.layers.Dense) and not is_attn:\n",
    "      pt_weight =pt_weight.transpose()\n",
    "\n",
    "    if isinstance(tf_component, (tf.keras.layers.Dense, tf.keras.layers.Conv3D)):\n",
    "        tf_component.kernel.assign(tf.Variable(pt_weight))\n",
    "\n",
    "        if pt_bias is not None:\n",
    "            tf_component.bias.assign(tf.Variable(pt_bias))\n",
    "\n",
    "    elif isinstance(tf_component, tf.keras.layers.LayerNormalization):\n",
    "\n",
    "        tf_component.gamma.assign(tf.Variable(pt_weight))\n",
    "\n",
    "        tf_component.beta.assign(tf.Variable(pt_bias))\n",
    "\n",
    "    elif isinstance(tf_component, (tf.Variable)):\n",
    "        tf_component.assign(tf.Variable(pt_weight))\n",
    "\n",
    "    else:\n",
    "        return tf.convert_to_tensor(pt_weight)\n",
    "        \n",
    "        \n",
    "\n",
    "    return tf_component\n",
    "\n",
    "\n",
    "def modify_swin_blocks(np_state_dict, pt_weights_prefix, tf_block):\n",
    "\n",
    "  for layer in tf_block:\n",
    "    if isinstance(layer, PatchMerging):\n",
    "      patch_merging_idx = f\"{pt_weights_prefix}.downsample\"\n",
    "\n",
    "      layer.reduction = modify_tf_block( layer.reduction,\n",
    "                          np_state_dict[f\"{patch_merging_idx}.reduction.weight\"])\n",
    "      layer.norm = modify_tf_block( layer.norm,\n",
    "                        np_state_dict[f\"{patch_merging_idx}.norm.weight\"],\n",
    "                        np_state_dict[f\"{patch_merging_idx}.norm.bias\"]\n",
    "                        )\n",
    "      \n",
    "  # Swin Layers\n",
    "  common_prefix = f\"{pt_weights_prefix}.blocks\"\n",
    "  block_idx = 0\n",
    "\n",
    "  for outer_layer in tf_block:\n",
    "\n",
    "      layernorm_idx = 1\n",
    "      mlp_layer_idx = 1\n",
    "\n",
    "      if isinstance(outer_layer, SwinTransformerBlock3D):\n",
    "          for inner_layer in outer_layer.layers:\n",
    "        \n",
    "              # Layer norm.\n",
    "              if isinstance(inner_layer, tf.keras.layers.LayerNormalization):\n",
    "                  layer_norm_prefix = (\n",
    "                      f\"{common_prefix}.{block_idx}.norm{layernorm_idx}\"\n",
    "                  )\n",
    "                  inner_layer.gamma.assign(\n",
    "                      tf.Variable(\n",
    "                          np_state_dict[f\"{layer_norm_prefix}.weight\"]\n",
    "                      )\n",
    "                  )\n",
    "\n",
    "\n",
    "\n",
    "                  inner_layer.beta.assign(\n",
    "                      tf.Variable(np_state_dict[f\"{layer_norm_prefix}.bias\"])\n",
    "                  )\n",
    "\n",
    "                  layernorm_idx += 1\n",
    "\n",
    "              # Window attention.\n",
    "              elif isinstance(inner_layer, WindowAttention3D):\n",
    "                  attn_prefix = f\"{common_prefix}.{block_idx}.attn\"\n",
    "\n",
    "                  # Relative position.\n",
    "                  inner_layer.relative_position_bias_table = (\n",
    "                      modify_tf_block(\n",
    "                          inner_layer.relative_position_bias_table,\n",
    "                          np_state_dict[\n",
    "                              f\"{attn_prefix}.relative_position_bias_table\"\n",
    "                          ] \n",
    "                      )\n",
    "                  )\n",
    "                  inner_layer.relative_position_index = (\n",
    "                      modify_tf_block(\n",
    "                          inner_layer.relative_position_index,\n",
    "                          np_state_dict[\n",
    "                              f\"{attn_prefix}.relative_position_index\"\n",
    "                          ]\n",
    "                      )\n",
    "                  )\n",
    "\n",
    "                  # QKV.\n",
    "                  inner_layer.qkv = modify_tf_block(\n",
    "                      inner_layer.qkv,\n",
    "                      np_state_dict[f\"{attn_prefix}.qkv.weight\"],\n",
    "                      np_state_dict[f\"{attn_prefix}.qkv.bias\"]\n",
    "                  )\n",
    "\n",
    "                  # Projection.\n",
    "                  inner_layer.proj = modify_tf_block(\n",
    "                      inner_layer.proj,\n",
    "                      np_state_dict[f\"{attn_prefix}.proj.weight\"],\n",
    "                      np_state_dict[f\"{attn_prefix}.proj.bias\"]\n",
    "                  )\n",
    "\n",
    "              # MLP.\n",
    "              elif isinstance(inner_layer, tf.keras.Model):\n",
    "                  mlp_prefix = f\"{common_prefix}.{block_idx}.mlp\"\n",
    "                  for mlp_layer in inner_layer.layers:\n",
    "                      if isinstance(mlp_layer, tf.keras.layers.Dense):\n",
    "                          mlp_layer = modify_tf_block(\n",
    "                              mlp_layer,\n",
    "                              np_state_dict[\n",
    "                                  f\"{mlp_prefix}.fc{mlp_layer_idx}.weight\"\n",
    "                              ],\n",
    "                              np_state_dict[\n",
    "                                  f\"{mlp_prefix}.fc{mlp_layer_idx}.bias\"\n",
    "                              ]\n",
    "                          )\n",
    "                          mlp_layer_idx += 1\n",
    "\n",
    "          block_idx += 1\n",
    "  return tf_block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_state_dict = pt_model.state_dict()\n",
    "np_state_dict = {k: np_state_dict[k].numpy() for k in np_state_dict}\n",
    "\n",
    "tf_model.projection.layers[0] = modify_tf_block(tf_model.projection.layers[0]\n",
    "        ,\n",
    "        np_state_dict[\"patch_embed.proj.weight\"],\n",
    "        np_state_dict[\"patch_embed.proj.bias\"])\n",
    "\n",
    "tf_model.projection.layers[1] = modify_tf_block(\n",
    "    tf_model.projection.layers[1],\n",
    "    np_state_dict[\"patch_embed.norm.weight\"],\n",
    "    np_state_dict[\"patch_embed.norm.bias\"])\n",
    "\n",
    "\n",
    "layer_normalization_idx = -1\n",
    "\n",
    "tf_model.layers[layer_normalization_idx] = modify_tf_block(\n",
    "    tf_model.layers[layer_normalization_idx] ,\n",
    "    np_state_dict[\"norm.weight\"],\n",
    "    np_state_dict[\"norm.bias\"]\n",
    "    )\n",
    "\n",
    "# swin layers\n",
    "for i in range(2, len(tf_model.layers) - 1):\n",
    "    _ = modify_swin_blocks(np_state_dict,\n",
    "                        f\"layers.{i-2}\",\n",
    "                        tf_model.layers[i].layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf_model(x_tf, training= False)\n",
    "z= pt_model(x_pt)\n",
    "\n",
    "y.shape, z.shape, \n",
    "y[:1,:1,:1,:1,:10], z[:1,:1,:1,:1,:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing the outputs\n",
    "np.testing.assert_allclose(y.numpy(), z.detach().numpy(), 1e-4, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"saving model \\n \\n\",\"=============================================\")\n",
    "\n",
    "tf_model.save(\"weights/model_tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model_loaded = tf.keras.models.load_model('weights/model_tiny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model_loaded.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_loaded  = tf_model_loaded(x_tf)\n",
    "y[:1,:1,:1,:1,:10], y_loaded[:1,:1,:1,:1,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(y.numpy(), y_loaded.numpy(), 1e-4, 1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape, y_loaded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18e9ffd7bdaebed3141c5f1e6e3ffefff8dc763f7fe0a2903683245d14d535a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
