{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from VideoSwinTransformer import *\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x():\n",
    "    x_pt = torch.rand((1,3,8,224,224))\n",
    "    x_np = x_pt.numpy()\n",
    "    x_tf = tf.convert_to_tensor(x_np)\n",
    "\n",
    "    return x_tf, x_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate model and Load PyTorch Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_method = model_configs.MODEL_MAP[\"swin_tiny_patch244_window877_kinetics400_1k\"]\n",
    "cfg = cfg_method()\n",
    "\n",
    "name = cfg[\"name\"]\n",
    "link = cfg['link']\n",
    "del cfg[\"name\"]\n",
    "del cfg['link']\n",
    "download_weight_command = f\"wget {link} -O {name}.pth\"\n",
    "os.system(download_weight_command)\n",
    "\n",
    "pt_model = SwinTransformer3D_pt(**cfg, isTest= True)\n",
    "tf_model = SwinTransformer3D(**cfg, isTest= True)\n",
    "x_tf, x_pt = get_x()\n",
    "\n",
    "\n",
    "basic_pt, y = tf_model(x_tf)\n",
    "\n",
    "basic_tf, z= pt_model(x_pt)\n",
    "\n",
    "checkpoint = torch.load(f'{name}.pth')\n",
    "\n",
    "\n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in checkpoint['state_dict'].items():\n",
    "    if 'backbone' in k:\n",
    "        name = k[9:]\n",
    "        new_state_dict[name] = v \n",
    "\n",
    "pt_model.load_state_dict(new_state_dict) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_transpose(w):\n",
    "    return w.transpose(2,3,4,1, 0)\n",
    "    \n",
    "\n",
    "def modify_tf_block( tf_component, pt_weight,  pt_bias = None, is_attn=False):\n",
    "    in_shape = pt_weight.shape\n",
    "\n",
    "    if isinstance(tf_component, tf.keras.layers.Conv3D) :\n",
    "      pt_weight = conv_transpose(pt_weight)\n",
    "\n",
    "    if isinstance(tf_component, tf.keras.layers.Dense) and not is_attn:\n",
    "      pt_weight =pt_weight.transpose()\n",
    "\n",
    "    if isinstance(tf_component, (tf.keras.layers.Dense, tf.keras.layers.Conv3D)):\n",
    "        tf_component.kernel.assign(tf.Variable(pt_weight))\n",
    "\n",
    "        if pt_bias is not None:\n",
    "            tf_component.bias.assign(tf.Variable(pt_bias))\n",
    "\n",
    "    elif isinstance(tf_component, tf.keras.layers.LayerNormalization):\n",
    "\n",
    "        tf_component.gamma.assign(tf.Variable(pt_weight))\n",
    "\n",
    "        tf_component.beta.assign(tf.Variable(pt_bias))\n",
    "\n",
    "    elif isinstance(tf_component, (tf.Variable)):\n",
    "        tf_component.assign(tf.Variable(pt_weight))\n",
    "\n",
    "    else:\n",
    "        return tf.convert_to_tensor(pt_weight)\n",
    "        \n",
    "        \n",
    "\n",
    "    return tf_component\n",
    "\n",
    "\n",
    "def modify_swin_blocks(np_state_dict, pt_weights_prefix, tf_block):\n",
    "\n",
    "  for layer in tf_block:\n",
    "    if isinstance(layer, PatchMerging):\n",
    "      patch_merging_idx = f\"{pt_weights_prefix}.downsample\"\n",
    "\n",
    "      layer.reduction = modify_tf_block( layer.reduction,\n",
    "                          np_state_dict[f\"{patch_merging_idx}.reduction.weight\"])\n",
    "      layer.norm = modify_tf_block( layer.norm,\n",
    "                        np_state_dict[f\"{patch_merging_idx}.norm.weight\"],\n",
    "                        np_state_dict[f\"{patch_merging_idx}.norm.bias\"]\n",
    "                        )\n",
    "      \n",
    "  # Swin Layers\n",
    "  common_prefix = f\"{pt_weights_prefix}.blocks\"\n",
    "  block_idx = 0\n",
    "\n",
    "  for outer_layer in tf_block:\n",
    "\n",
    "      layernorm_idx = 1\n",
    "      mlp_layer_idx = 1\n",
    "\n",
    "      if isinstance(outer_layer, SwinTransformerBlock3D):\n",
    "          for inner_layer in outer_layer.layers:\n",
    "        \n",
    "              # Layer norm.\n",
    "              if isinstance(inner_layer, tf.keras.layers.LayerNormalization):\n",
    "                  layer_norm_prefix = (\n",
    "                      f\"{common_prefix}.{block_idx}.norm{layernorm_idx}\"\n",
    "                  )\n",
    "                  inner_layer.gamma.assign(\n",
    "                      tf.Variable(\n",
    "                          np_state_dict[f\"{layer_norm_prefix}.weight\"]\n",
    "                      )\n",
    "                  )\n",
    "\n",
    "\n",
    "\n",
    "                  inner_layer.beta.assign(\n",
    "                      tf.Variable(np_state_dict[f\"{layer_norm_prefix}.bias\"])\n",
    "                  )\n",
    "\n",
    "                  layernorm_idx += 1\n",
    "\n",
    "              # Window attention.\n",
    "              elif isinstance(inner_layer, WindowAttention3D):\n",
    "                  attn_prefix = f\"{common_prefix}.{block_idx}.attn\"\n",
    "\n",
    "                  # Relative position.\n",
    "                  inner_layer.relative_position_bias_table = (\n",
    "                      modify_tf_block(\n",
    "                          inner_layer.relative_position_bias_table,\n",
    "                          np_state_dict[\n",
    "                              f\"{attn_prefix}.relative_position_bias_table\"\n",
    "                          ] \n",
    "                      )\n",
    "                  )\n",
    "                  inner_layer.relative_position_index = (\n",
    "                      modify_tf_block(\n",
    "                          inner_layer.relative_position_index,\n",
    "                          np_state_dict[\n",
    "                              f\"{attn_prefix}.relative_position_index\"\n",
    "                          ]\n",
    "                      )\n",
    "                  )\n",
    "\n",
    "                  # QKV.\n",
    "                  inner_layer.qkv = modify_tf_block(\n",
    "                      inner_layer.qkv,\n",
    "                      np_state_dict[f\"{attn_prefix}.qkv.weight\"],\n",
    "                      np_state_dict[f\"{attn_prefix}.qkv.bias\"]\n",
    "                  )\n",
    "\n",
    "                  # Projection.\n",
    "                  inner_layer.proj = modify_tf_block(\n",
    "                      inner_layer.proj,\n",
    "                      np_state_dict[f\"{attn_prefix}.proj.weight\"],\n",
    "                      np_state_dict[f\"{attn_prefix}.proj.bias\"]\n",
    "                  )\n",
    "\n",
    "              # MLP.\n",
    "              elif isinstance(inner_layer, tf.keras.Model):\n",
    "                  mlp_prefix = f\"{common_prefix}.{block_idx}.mlp\"\n",
    "                  for mlp_layer in inner_layer.layers:\n",
    "                      if isinstance(mlp_layer, tf.keras.layers.Dense):\n",
    "                          mlp_layer = modify_tf_block(\n",
    "                              mlp_layer,\n",
    "                              np_state_dict[\n",
    "                                  f\"{mlp_prefix}.fc{mlp_layer_idx}.weight\"\n",
    "                              ],\n",
    "                              np_state_dict[\n",
    "                                  f\"{mlp_prefix}.fc{mlp_layer_idx}.bias\"\n",
    "                              ]\n",
    "                          )\n",
    "                          mlp_layer_idx += 1\n",
    "\n",
    "          block_idx += 1\n",
    "  return tf_block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_state_dict = pt_model.state_dict()\n",
    "np_state_dict = {k: np_state_dict[k].numpy() for k in np_state_dict}\n",
    "\n",
    "tf_model.projection.layers[0] = modify_tf_block(tf_model.projection.layers[0]\n",
    "        ,\n",
    "        np_state_dict[\"patch_embed.proj.weight\"],\n",
    "        np_state_dict[\"patch_embed.proj.bias\"])\n",
    "\n",
    "tf_model.projection.layers[1] = modify_tf_block(\n",
    "    tf_model.projection.layers[1],\n",
    "    np_state_dict[\"patch_embed.norm.weight\"],\n",
    "    np_state_dict[\"patch_embed.norm.bias\"])\n",
    "\n",
    "\n",
    "layer_normalization_idx = -1\n",
    "\n",
    "tf_model.layers[layer_normalization_idx] = modify_tf_block(\n",
    "    tf_model.layers[layer_normalization_idx] ,\n",
    "    np_state_dict[\"norm.weight\"],\n",
    "    np_state_dict[\"norm.bias\"]\n",
    "    )\n",
    "\n",
    "# swin layers\n",
    "for i in range(2, len(tf_model.layers) - 1):\n",
    "    _ = modify_swin_blocks(np_state_dict,\n",
    "                        f\"layers.{i-2}\",\n",
    "                        tf_model.layers[i].layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_pt, y = tf_model(x_tf)\n",
    "basic_tf, z= pt_model(x_pt)\n",
    "\n",
    "y.shape, z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in basic_pt:\n",
    "    print(\"--------------\",layer, \"-------------\\n TF: \", basic_tf[layer].detach().numpy(), \"\\n PT: \", basic_pt[layer].numpy(), \"\\n\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PT basic layer outputs comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempts = 10\n",
    "# x_pt = torch.rand((1,3,8,224,224))\n",
    "\n",
    "# outputs = []\n",
    "\n",
    "# for i in range(attempts):\n",
    "#     layer_out , result = pt_model(x_pt)\n",
    "#     outputs.append(layer_out)\n",
    "\n",
    "# i = 0\n",
    "# for layer in outputs[0]:\n",
    "#     print(\"--------------\", layer, \"---------------\")\n",
    "#     for idx, layer_out in enumerate(outputs) :\n",
    "#         print(f\"attempt {idx} : \",layer_out[layer].detach().numpy(), \"\\n\")\n",
    "#     print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ed124f2b279c186db57d0ede455df2b109954ad4fa1a5a2c59adb747c894aa2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
