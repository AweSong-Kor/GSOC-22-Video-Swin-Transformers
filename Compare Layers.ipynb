{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from VideoSwinTransformer import *\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x():\n",
    "    x_pt = torch.rand((1,3,8,224,224))\n",
    "    x_np = x_pt.numpy()\n",
    "    x_tf = tf.convert_to_tensor(x_np)\n",
    "\n",
    "    return x_tf, x_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate model and Load PyTorch Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python396\\lib\\site-packages\\torch\\functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_intermediate_layer_getter import IntermediateLayerGetter as MidGetter\n",
    "\n",
    "\n",
    "cfg_method = model_configs.MODEL_MAP[\"swin_tiny_patch244_window877_kinetics400_1k\"]\n",
    "cfg = cfg_method()\n",
    "\n",
    "name = cfg[\"name\"]\n",
    "link = cfg['link']\n",
    "del cfg[\"name\"]\n",
    "del cfg['link']\n",
    "# download_weight_command = f\"wget {link} -O {name}.pth\"\n",
    "# os.system(download_weight_command)\n",
    "\n",
    "pt_model = SwinTransformer3D_pt(**cfg, isTest= True)\n",
    "tf_model = SwinTransformer3D(**cfg, isTest= True)\n",
    "x_tf, x_pt = get_x()\n",
    "\n",
    "\n",
    "basic_pt, y = tf_model(x_tf)\n",
    "\n",
    "basic_tf, z= pt_model(x_pt)\n",
    "\n",
    "checkpoint = torch.load(f'{name}.pth')\n",
    "\n",
    "\n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in checkpoint['state_dict'].items():\n",
    "    if 'backbone' in k:\n",
    "        name = k[9:]\n",
    "        new_state_dict[name] = v \n",
    "\n",
    "pt_model.load_state_dict(new_state_dict) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_transpose(w):\n",
    "    return w.transpose(2,3,4,1, 0)\n",
    "    \n",
    "\n",
    "def modify_tf_block( tf_component, pt_weight,  pt_bias = None, is_attn=False):\n",
    "    in_shape = pt_weight.shape\n",
    "\n",
    "    if isinstance(tf_component, tf.keras.layers.Conv3D) :\n",
    "      pt_weight = conv_transpose(pt_weight)\n",
    "\n",
    "    if isinstance(tf_component, tf.keras.layers.Dense) and not is_attn:\n",
    "      pt_weight =pt_weight.transpose()\n",
    "\n",
    "    if isinstance(tf_component, (tf.keras.layers.Dense, tf.keras.layers.Conv3D)):\n",
    "        tf_component.kernel.assign(tf.Variable(pt_weight))\n",
    "\n",
    "        if pt_bias is not None:\n",
    "            tf_component.bias.assign(tf.Variable(pt_bias))\n",
    "\n",
    "    elif isinstance(tf_component, tf.keras.layers.LayerNormalization):\n",
    "\n",
    "        tf_component.gamma.assign(tf.Variable(pt_weight))\n",
    "\n",
    "        tf_component.beta.assign(tf.Variable(pt_bias))\n",
    "\n",
    "    elif isinstance(tf_component, (tf.Variable)):\n",
    "        tf_component.assign(tf.Variable(pt_weight))\n",
    "\n",
    "    else:\n",
    "        return tf.convert_to_tensor(pt_weight)\n",
    "        \n",
    "        \n",
    "\n",
    "    return tf_component\n",
    "\n",
    "\n",
    "def modify_swin_blocks(np_state_dict, pt_weights_prefix, tf_block):\n",
    "\n",
    "  for layer in tf_block:\n",
    "    if isinstance(layer, PatchMerging):\n",
    "      patch_merging_idx = f\"{pt_weights_prefix}.downsample\"\n",
    "\n",
    "      layer.reduction = modify_tf_block( layer.reduction,\n",
    "                          np_state_dict[f\"{patch_merging_idx}.reduction.weight\"])\n",
    "      layer.norm = modify_tf_block( layer.norm,\n",
    "                        np_state_dict[f\"{patch_merging_idx}.norm.weight\"],\n",
    "                        np_state_dict[f\"{patch_merging_idx}.norm.bias\"]\n",
    "                        )\n",
    "      \n",
    "  # Swin Layers\n",
    "  common_prefix = f\"{pt_weights_prefix}.blocks\"\n",
    "  block_idx = 0\n",
    "\n",
    "  for outer_layer in tf_block:\n",
    "\n",
    "      layernorm_idx = 1\n",
    "      mlp_layer_idx = 1\n",
    "\n",
    "      if isinstance(outer_layer, SwinTransformerBlock3D):\n",
    "          for inner_layer in outer_layer.layers:\n",
    "        \n",
    "              # Layer norm.\n",
    "              if isinstance(inner_layer, tf.keras.layers.LayerNormalization):\n",
    "                  layer_norm_prefix = (\n",
    "                      f\"{common_prefix}.{block_idx}.norm{layernorm_idx}\"\n",
    "                  )\n",
    "                  inner_layer.gamma.assign(\n",
    "                      tf.Variable(\n",
    "                          np_state_dict[f\"{layer_norm_prefix}.weight\"]\n",
    "                      )\n",
    "                  )\n",
    "\n",
    "\n",
    "\n",
    "                  inner_layer.beta.assign(\n",
    "                      tf.Variable(np_state_dict[f\"{layer_norm_prefix}.bias\"])\n",
    "                  )\n",
    "\n",
    "                  layernorm_idx += 1\n",
    "\n",
    "              # Window attention.\n",
    "              elif isinstance(inner_layer, WindowAttention3D):\n",
    "                  attn_prefix = f\"{common_prefix}.{block_idx}.attn\"\n",
    "\n",
    "                  # Relative position.\n",
    "                  inner_layer.relative_position_bias_table = (\n",
    "                      modify_tf_block(\n",
    "                          inner_layer.relative_position_bias_table,\n",
    "                          np_state_dict[\n",
    "                              f\"{attn_prefix}.relative_position_bias_table\"\n",
    "                          ] \n",
    "                      )\n",
    "                  )\n",
    "                  inner_layer.relative_position_index = (\n",
    "                      modify_tf_block(\n",
    "                          inner_layer.relative_position_index,\n",
    "                          np_state_dict[\n",
    "                              f\"{attn_prefix}.relative_position_index\"\n",
    "                          ]\n",
    "                      )\n",
    "                  )\n",
    "\n",
    "                  # QKV.\n",
    "                  inner_layer.qkv = modify_tf_block(\n",
    "                      inner_layer.qkv,\n",
    "                      np_state_dict[f\"{attn_prefix}.qkv.weight\"],\n",
    "                      np_state_dict[f\"{attn_prefix}.qkv.bias\"]\n",
    "                  )\n",
    "\n",
    "                  # Projection.\n",
    "                  inner_layer.proj = modify_tf_block(\n",
    "                      inner_layer.proj,\n",
    "                      np_state_dict[f\"{attn_prefix}.proj.weight\"],\n",
    "                      np_state_dict[f\"{attn_prefix}.proj.bias\"]\n",
    "                  )\n",
    "\n",
    "              # MLP.\n",
    "              elif isinstance(inner_layer, tf.keras.Model):\n",
    "                  mlp_prefix = f\"{common_prefix}.{block_idx}.mlp\"\n",
    "                  for mlp_layer in inner_layer.layers:\n",
    "                      if isinstance(mlp_layer, tf.keras.layers.Dense):\n",
    "                          mlp_layer = modify_tf_block(\n",
    "                              mlp_layer,\n",
    "                              np_state_dict[\n",
    "                                  f\"{mlp_prefix}.fc{mlp_layer_idx}.weight\"\n",
    "                              ],\n",
    "                              np_state_dict[\n",
    "                                  f\"{mlp_prefix}.fc{mlp_layer_idx}.bias\"\n",
    "                              ]\n",
    "                          )\n",
    "                          mlp_layer_idx += 1\n",
    "\n",
    "          block_idx += 1\n",
    "  return tf_block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_state_dict = pt_model.state_dict()\n",
    "np_state_dict = {k: np_state_dict[k].numpy() for k in np_state_dict}\n",
    "\n",
    "tf_model.projection.layers[0] = modify_tf_block(tf_model.projection.layers[0]\n",
    "        ,\n",
    "        np_state_dict[\"patch_embed.proj.weight\"],\n",
    "        np_state_dict[\"patch_embed.proj.bias\"])\n",
    "\n",
    "tf_model.projection.layers[1] = modify_tf_block(\n",
    "    tf_model.projection.layers[1],\n",
    "    np_state_dict[\"patch_embed.norm.weight\"],\n",
    "    np_state_dict[\"patch_embed.norm.bias\"])\n",
    "\n",
    "\n",
    "layer_normalization_idx = -1\n",
    "\n",
    "tf_model.layers[layer_normalization_idx] = modify_tf_block(\n",
    "    tf_model.layers[layer_normalization_idx] ,\n",
    "    np_state_dict[\"norm.weight\"],\n",
    "    np_state_dict[\"norm.bias\"]\n",
    "    )\n",
    "\n",
    "# swin layers\n",
    "for i in range(2, len(tf_model.layers) - 1):\n",
    "    _ = modify_swin_blocks(np_state_dict,\n",
    "                        f\"layers.{i-2}\",\n",
    "                        tf_model.layers[i].layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 768, 4, 7, 7]), torch.Size([1, 768, 4, 7, 7]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_pt, y = tf_model(x_tf)\n",
    "basic_tf, z= pt_model(x_pt)\n",
    "\n",
    "y.shape, z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- PatchEmbed -------------\n",
      " TF:  [[[[[0.4694879  0.7320849  0.668663   0.6526338  0.8030871  0.9637686\n",
      "     0.8512107  0.37930787 0.40388405 0.52170944]]]]] \n",
      " PT:  [[[[[0.46948713 0.73208445 0.66866344 0.6526342  0.8030863  0.9637688\n",
      "     0.8512116  0.37930882 0.40388483 0.5217101 ]]]]] \n",
      "\n",
      "-------------- basic layer1 -------------\n",
      " TF:  [[[[[ 0.05090611  0.6119843   0.19952458 -0.26986203  0.43620658\n",
      "      0.1511359  -0.08458254  0.49404496  0.46321994 -0.40098152]]]]] \n",
      " PT:  [[[[[ 0.03283307  0.5923909   0.1710603  -0.2625764   0.43319678\n",
      "      0.14306492 -0.06784578  0.4730916   0.45867777 -0.4456384 ]]]]] \n",
      "\n",
      "-------------- basic layer2 -------------\n",
      " TF:  [[[[[-0.56932425 -0.5508435  -0.63364273 -0.47360727 -0.49055067\n",
      "     -0.1942482  -0.56187135 -0.3953247  -0.44955727 -0.4444399 ]]]]] \n",
      " PT:  [[[[[-0.5480101  -0.5739908  -0.65543723 -0.46185446 -0.46572882\n",
      "     -0.23131174 -0.5980027  -0.35696417 -0.46548173 -0.46797377]]]]] \n",
      "\n",
      "-------------- basic layer3 -------------\n",
      " TF:  [[[[[0.12329817 0.03577615 0.34940696 0.30996734 0.13353582 0.11097425\n",
      "     0.29220837]]]]] \n",
      " PT:  [[[[[0.12134396 0.04387612 0.32654747 0.35945955 0.13875632 0.14307272\n",
      "     0.29943565]]]]] \n",
      "\n",
      "-------------- basic layer4 -------------\n",
      " TF:  [[[[[ 1.1211858   1.4164813   0.07958797  1.3312504   1.0080247\n",
      "     -1.1421504   1.2697966 ]]]]] \n",
      " PT:  [[[[[ 1.0261139  1.7621958 -0.0244898  1.2888587  0.9385733 -1.1475196\n",
      "      1.1781236]]]]] \n",
      "\n",
      "-------------- Final Output -------------\n",
      " TF:  [[[[[ 0.2863748   0.8544751   0.01155449  0.31492722  0.3297903\n",
      "     -0.15176666  0.3873085 ]]]]] \n",
      " PT:  [[[[[ 0.2686896   1.1887608  -0.00528939  0.29490423  0.32000446\n",
      "     -0.16444305  0.3612858 ]]]]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for layer in basic_pt:\n",
    "    print(\"--------------\",layer, \"-------------\\n TF: \", basic_tf[layer].detach().numpy(), \"\\n PT: \", basic_pt[layer].numpy(), \"\\n\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PT basic layer outputs comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- PatchEmbed ---------------\n",
      "attempt 0 :  [[[[[0.34786695 1.3482637  0.3097511  0.5879744  0.7180053  0.36379492\n",
      "     0.46565378 0.6328912  0.41669297 1.0991832 ]]]]] \n",
      "\n",
      "attempt 1 :  [[[[[0.34786695 1.3482637  0.3097511  0.5879744  0.7180053  0.36379492\n",
      "     0.46565378 0.6328912  0.41669297 1.0991832 ]]]]] \n",
      "\n",
      "attempt 2 :  [[[[[0.34786695 1.3482637  0.3097511  0.5879744  0.7180053  0.36379492\n",
      "     0.46565378 0.6328912  0.41669297 1.0991832 ]]]]] \n",
      "\n",
      "attempt 3 :  [[[[[0.34786695 1.3482637  0.3097511  0.5879744  0.7180053  0.36379492\n",
      "     0.46565378 0.6328912  0.41669297 1.0991832 ]]]]] \n",
      "\n",
      "attempt 4 :  [[[[[0.34786695 1.3482637  0.3097511  0.5879744  0.7180053  0.36379492\n",
      "     0.46565378 0.6328912  0.41669297 1.0991832 ]]]]] \n",
      "\n",
      "attempt 5 :  [[[[[0.34786695 1.3482637  0.3097511  0.5879744  0.7180053  0.36379492\n",
      "     0.46565378 0.6328912  0.41669297 1.0991832 ]]]]] \n",
      "\n",
      "attempt 6 :  [[[[[0.34786695 1.3482637  0.3097511  0.5879744  0.7180053  0.36379492\n",
      "     0.46565378 0.6328912  0.41669297 1.0991832 ]]]]] \n",
      "\n",
      "attempt 7 :  [[[[[0.34786695 1.3482637  0.3097511  0.5879744  0.7180053  0.36379492\n",
      "     0.46565378 0.6328912  0.41669297 1.0991832 ]]]]] \n",
      "\n",
      "attempt 8 :  [[[[[0.34786695 1.3482637  0.3097511  0.5879744  0.7180053  0.36379492\n",
      "     0.46565378 0.6328912  0.41669297 1.0991832 ]]]]] \n",
      "\n",
      "attempt 9 :  [[[[[0.34786695 1.3482637  0.3097511  0.5879744  0.7180053  0.36379492\n",
      "     0.46565378 0.6328912  0.41669297 1.0991832 ]]]]] \n",
      "\n",
      "\n",
      "-------------- basic layer1 ---------------\n",
      "attempt 0 :  [[[[[ 0.02874935  0.5204139   0.7410587  -0.91241926  0.35252276\n",
      "      0.1836612   0.2674764   0.20678097  0.40824336  0.2765409 ]]]]] \n",
      "\n",
      "attempt 1 :  [[[[[ 0.02874935  0.5204139   0.7410587  -0.91241926  0.35252276\n",
      "      0.1836612   0.2674764   0.20678097  0.40824336  0.2765409 ]]]]] \n",
      "\n",
      "attempt 2 :  [[[[[ 0.20285437  0.7549669   0.9701253  -0.66940004  0.52613056\n",
      "      0.46828336  0.45389056  0.3244416   0.6897961   0.36603704]]]]] \n",
      "\n",
      "attempt 3 :  [[[[[ 0.02874935  0.5204139   0.7410587  -0.91241926  0.35252276\n",
      "      0.1836612   0.2674764   0.20678097  0.40824336  0.2765409 ]]]]] \n",
      "\n",
      "attempt 4 :  [[[[[ 0.02874935  0.5204139   0.7410587  -0.91241926  0.35252276\n",
      "      0.1836612   0.2674764   0.20678097  0.40824336  0.2765409 ]]]]] \n",
      "\n",
      "attempt 5 :  [[[[[ 0.02874935  0.5204139   0.7410587  -0.91241926  0.35252276\n",
      "      0.1836612   0.2674764   0.20678097  0.40824336  0.2765409 ]]]]] \n",
      "\n",
      "attempt 6 :  [[[[[ 0.02874935  0.5204139   0.7410587  -0.91241926  0.35252276\n",
      "      0.1836612   0.2674764   0.20678097  0.40824336  0.2765409 ]]]]] \n",
      "\n",
      "attempt 7 :  [[[[[ 0.02874935  0.5204139   0.7410587  -0.91241926  0.35252276\n",
      "      0.1836612   0.2674764   0.20678097  0.40824336  0.2765409 ]]]]] \n",
      "\n",
      "attempt 8 :  [[[[[ 0.02874935  0.5204139   0.7410587  -0.91241926  0.35252276\n",
      "      0.1836612   0.2674764   0.20678097  0.40824336  0.2765409 ]]]]] \n",
      "\n",
      "attempt 9 :  [[[[[ 0.02874935  0.5204139   0.7410587  -0.91241926  0.35252276\n",
      "      0.1836612   0.2674764   0.20678097  0.40824336  0.2765409 ]]]]] \n",
      "\n",
      "\n",
      "-------------- basic layer2 ---------------\n",
      "attempt 0 :  [[[[[-0.6969895  -0.6506766  -0.30246958 -0.90046763 -0.28024936\n",
      "     -0.47385925 -0.06908301 -0.14925662 -0.4143948   0.09622303]]]]] \n",
      "\n",
      "attempt 1 :  [[[[[-0.7089186  -0.842417   -0.63898253 -0.9970333  -0.45706323\n",
      "     -0.6955551  -0.2023049  -0.5059628  -0.6955781  -0.08077545]]]]] \n",
      "\n",
      "attempt 2 :  [[[[[-0.81583554 -0.8049537  -0.67977023 -0.9527124  -0.6039879\n",
      "     -0.6646819  -0.1747806  -0.3023883  -0.57330793  0.02353256]]]]] \n",
      "\n",
      "attempt 3 :  [[[[[-0.7089186  -0.842417   -0.63898253 -0.9970333  -0.45706323\n",
      "     -0.6955551  -0.2023049  -0.5059628  -0.6955781  -0.08077545]]]]] \n",
      "\n",
      "attempt 4 :  [[[[[-0.7089186  -0.842417   -0.63898253 -0.9970333  -0.45706323\n",
      "     -0.6955551  -0.2023049  -0.5059628  -0.6955781  -0.08077545]]]]] \n",
      "\n",
      "attempt 5 :  [[[[[-0.7089186  -0.842417   -0.63898253 -0.9970333  -0.45706323\n",
      "     -0.6955551  -0.2023049  -0.5059628  -0.6955781  -0.08077545]]]]] \n",
      "\n",
      "attempt 6 :  [[[[[-0.7089186  -0.842417   -0.63898253 -0.9970333  -0.45706323\n",
      "     -0.6955551  -0.2023049  -0.5059628  -0.6955781  -0.08077545]]]]] \n",
      "\n",
      "attempt 7 :  [[[[[-0.7089186  -0.842417   -0.63898253 -0.9970333  -0.45706323\n",
      "     -0.6955551  -0.2023049  -0.5059628  -0.6955781  -0.08077545]]]]] \n",
      "\n",
      "attempt 8 :  [[[[[-0.7089186  -0.842417   -0.63898253 -0.9970333  -0.45706323\n",
      "     -0.6955551  -0.2023049  -0.5059628  -0.6955781  -0.08077545]]]]] \n",
      "\n",
      "attempt 9 :  [[[[[-0.7089186  -0.842417   -0.63898253 -0.9970333  -0.45706323\n",
      "     -0.6955551  -0.2023049  -0.5059628  -0.6955781  -0.08077545]]]]] \n",
      "\n",
      "\n",
      "-------------- basic layer3 ---------------\n",
      "attempt 0 :  [[[[[0.12352973 0.16009872 0.2732899  0.3577837  0.34794948 0.26919603\n",
      "     0.24885437]]]]] \n",
      "\n",
      "attempt 1 :  [[[[[0.12421016 0.26723024 0.30763075 0.23655435 0.21753527 0.12385255\n",
      "     0.2877018 ]]]]] \n",
      "\n",
      "attempt 2 :  [[[[[ 0.26033133 -0.07064914  0.0681721  -0.12111059 -0.0686345\n",
      "     -0.16848287 -0.05421616]]]]] \n",
      "\n",
      "attempt 3 :  [[[[[ 0.12245736  0.15633032  0.31834382  0.25104317  0.07703601\n",
      "     -0.29759952  0.09719568]]]]] \n",
      "\n",
      "attempt 4 :  [[[[[0.12232161 0.32828328 0.29406202 0.41891158 0.49354544 0.4142196\n",
      "     0.32713038]]]]] \n",
      "\n",
      "attempt 5 :  [[[[[0.12721583 0.29153615 0.31178603 0.24130729 0.26629603 0.18994987\n",
      "     0.29102534]]]]] \n",
      "\n",
      "attempt 6 :  [[[[[0.12421016 0.26723024 0.30763075 0.23655435 0.21753527 0.12385255\n",
      "     0.2877018 ]]]]] \n",
      "\n",
      "attempt 7 :  [[[[[0.12421016 0.26723024 0.30763075 0.23655435 0.21753527 0.12385255\n",
      "     0.2877018 ]]]]] \n",
      "\n",
      "attempt 8 :  [[[[[0.12421016 0.26723024 0.30763075 0.23655435 0.21753527 0.12385255\n",
      "     0.2877018 ]]]]] \n",
      "\n",
      "attempt 9 :  [[[[[0.12421016 0.26723024 0.30763075 0.23655435 0.21753527 0.12385255\n",
      "     0.2877018 ]]]]] \n",
      "\n",
      "\n",
      "-------------- basic layer4 ---------------\n",
      "attempt 0 :  [[[[[0.5851724  0.4299652  0.7020315  0.52273196 0.46946347 0.47620356\n",
      "     0.52109766]]]]] \n",
      "\n",
      "attempt 1 :  [[[[[ 0.78921527 -0.09091243  1.0083743   0.8824545  -0.06807946\n",
      "      1.0871199   0.94353676]]]]] \n",
      "\n",
      "attempt 2 :  [[[[[-0.7353331  -1.3588221  -1.984368   -1.1982936  -0.6566695\n",
      "      1.0360811   0.22047338]]]]] \n",
      "\n",
      "attempt 3 :  [[[[[ 1.1725429  1.0034394  1.3169413 -0.9058256  1.1469374  1.854596\n",
      "     -0.7601695]]]]] \n",
      "\n",
      "attempt 4 :  [[[[[ 0.6245653   0.17178224  0.76982576 -0.61058974 -0.8636098\n",
      "      0.159116    0.7757232 ]]]]] \n",
      "\n",
      "attempt 5 :  [[[[[ 1.1082433e+00  6.7489874e-04  1.2928040e+00  1.1603822e+00\n",
      "     -7.0044659e-02  8.1908023e-01  1.2350914e+00]]]]] \n",
      "\n",
      "attempt 6 :  [[[[[ 1.0082866   0.10964698  1.153026    1.0392085  -0.05445299\n",
      "      0.48150247  1.1149275 ]]]]] \n",
      "\n",
      "attempt 7 :  [[[[[ 1.1211787   0.15678632  1.30183     1.1704462  -0.01118003\n",
      "      0.9856967   1.2480015 ]]]]] \n",
      "\n",
      "attempt 8 :  [[[[[ 1.005631   -1.0678664   1.1926922   1.00967    -0.7847847\n",
      "     -0.30308032  1.136094  ]]]]] \n",
      "\n",
      "attempt 9 :  [[[[[ 1.1211787   0.15678632  1.30183     1.1704462  -0.01118003\n",
      "      0.9856967   1.2480015 ]]]]] \n",
      "\n",
      "\n",
      "-------------- Final Output ---------------\n",
      "attempt 0 :  [[[[[0.19705424 0.28855228 0.22813216 0.11970793 0.12435371 0.16555503\n",
      "     0.18768305]]]]] \n",
      "\n",
      "attempt 1 :  [[[[[ 0.19651896 -0.01594023  0.2610541   0.23999368 -0.01463667\n",
      "      0.41808933  0.27461976]]]]] \n",
      "\n",
      "attempt 2 :  [[[[[-0.09234244 -0.17956594 -0.21222599 -0.16506007 -0.12058839\n",
      "      0.58912265  0.05276316]]]]] \n",
      "\n",
      "attempt 3 :  [[[[[ 0.2904761   0.4283075   0.34855464 -0.10889576  0.66738945\n",
      "      1.1012324  -0.10837016]]]]] \n",
      "\n",
      "attempt 4 :  [[[[[ 0.18980959  0.12165113  0.281448   -0.19638428 -0.3396316\n",
      "      0.1142093   0.32101852]]]]] \n",
      "\n",
      "attempt 5 :  [[[[[ 0.27715003 -0.00099608  0.32860282  0.31814128 -0.0177932\n",
      "      0.36049837  0.3671915 ]]]]] \n",
      "\n",
      "attempt 6 :  [[[[[ 0.29713294  0.0416063   0.35744     0.35496145 -0.03923065\n",
      "      0.4401446   0.4184375 ]]]]] \n",
      "\n",
      "attempt 7 :  [[[[[ 0.2803185   0.02935861  0.33145222  0.3205813  -0.00412033\n",
      "      0.5559443   0.37009865]]]]] \n",
      "\n",
      "attempt 8 :  [[[[[ 0.2540392  -0.14392428  0.31073302  0.27696902 -0.13993157\n",
      "     -0.08407178  0.33822617]]]]] \n",
      "\n",
      "attempt 9 :  [[[[[ 0.2803185   0.02935861  0.33145222  0.3205813  -0.00412033\n",
      "      0.5559443   0.37009865]]]]] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attempts = 10\n",
    "x_pt = torch.rand((1,3,8,224,224))\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for i in range(attempts):\n",
    "    layer_out , result = pt_model(x_pt)\n",
    "    outputs.append(layer_out)\n",
    "\n",
    "i = 0\n",
    "for layer in outputs[0]:\n",
    "    print(\"--------------\", layer, \"---------------\")\n",
    "    for idx, layer_out in enumerate(outputs) :\n",
    "        print(f\"attempt {idx} : \",layer_out[layer].detach().numpy(), \"\\n\")\n",
    "    print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ed124f2b279c186db57d0ede455df2b109954ad4fa1a5a2c59adb747c894aa2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
