{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices(device_type = 'GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ucf101'\n",
    "\n",
    "ucf101 = tfds.builder(dataset_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = tfds.download.DownloadConfig(verify_ssl=False)\n",
    "# ucf101.download_and_prepare(download_config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 101\n",
      "Number of examples for train: 9537\n",
      "Number of examples for test: 3783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_classes = ucf101.info.features['label'].num_classes\n",
    "num_examples = {\n",
    "    name: split.num_examples\n",
    "    for name, split in ucf101.info.splits.items()\n",
    "}\n",
    "\n",
    "print('Number of classes:', num_classes)\n",
    "print('Number of examples for train:', num_examples['train'])\n",
    "print('Number of examples for test:', num_examples['test'])\n",
    "print()\n",
    "\n",
    "# ucf101.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the training and evaluation datasets.\n",
    "batch_size = 2\n",
    "num_frames = 32\n",
    "resolution = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def format_features(features):\n",
    "  # print(\"features\", features)\n",
    "  video = features['video']\n",
    "  print(video.shape, \"start\", features)\n",
    "\n",
    "\n",
    "  total_frames = video.shape[1]\n",
    "  if total_frames == None:\n",
    "    total_frames = num_frames\n",
    "  frames = num_frames\n",
    "\n",
    "  start_idx = random.randint(0, total_frames - frames )\n",
    "  video = video[:,start_idx:start_idx+32]\n",
    "  print(video.shape, video.shape[1])\n",
    "\n",
    "  if video.shape[1] is None or video.shape[1]  < 32:\n",
    "    video = tf.random.normal((batch_size, 32, 256, 256, 3))\n",
    "    \n",
    "  video = tf.reshape(video, [-1, video.shape[2], video.shape[3], 3])\n",
    "  print(\"reshape\",video.shape)\n",
    "\n",
    "  \n",
    "  video = tf.image.resize(video, (resolution, resolution))\n",
    "  video = tf.reshape(video, [-1, num_frames, resolution, resolution, 3])\n",
    "  video = tf.transpose(video, perm=(0,4,1,2,3))\n",
    "\n",
    "  print(\"transpose\",video.shape)\n",
    "  \n",
    "  # video = tf.image.random_crop(video, (-1,32,224,224,3))\n",
    "\n",
    "  # if video.shape[0] is not None:\n",
    "  #   videos = tf.unstack(video)\n",
    "  #   for video, i in enumerate(videos):\n",
    "  #     isFlip = random.choice([\"flip\", \"don't flip\"])\n",
    "  #     if isFlip == \"flip\":\n",
    "  #         videos[i]= tf.image.flip_left_right(video)\n",
    "  #   video = tf.stack(videos)\n",
    "  video = tf.image.per_image_standardization(video)\n",
    "\n",
    "  print(video.shape)\n",
    "  \n",
    "  label = tf.one_hot(features['label'], num_classes)\n",
    "  return (video, label)\n",
    "\n",
    "\n",
    "# format_features(features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-25 12:01:04.347007: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-25 12:01:05.031525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15401 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0001:00:00.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 256, 256, 3) start {'label': <tf.Tensor 'args_0:0' shape=(None,) dtype=int64>, 'video': <tf.Tensor 'args_1:0' shape=(None, None, 256, 256, 3) dtype=uint8>}\n",
      "(None, None, 256, 256, 3) None\n",
      "reshape (64, 256, 256, 3)\n",
      "transpose (2, 3, 32, 224, 224)\n",
      "(2, 3, 32, 224, 224)\n",
      "(None, None, 256, 256, 3) start {'label': <tf.Tensor 'args_0:0' shape=(None,) dtype=int64>, 'video': <tf.Tensor 'args_1:0' shape=(None, None, 256, 256, 3) dtype=uint8>}\n",
      "(None, None, 256, 256, 3) None\n",
      "reshape (64, 256, 256, 3)\n",
      "transpose (2, 3, 32, 224, 224)\n",
      "(2, 3, 32, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ucf101.as_dataset(\n",
    "    split='train',\n",
    "    batch_size=batch_size,\n",
    "    shuffle_files=True)\n",
    "train_dataset = train_dataset.map(\n",
    "    format_features,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.repeat()\n",
    "train_dataset = train_dataset.prefetch(2)\n",
    "\n",
    "test_dataset = ucf101.as_dataset(\n",
    "    split='test',\n",
    "    batch_size=batch_size)\n",
    "test_dataset = test_dataset.map(\n",
    "    format_features,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    deterministic=True)\n",
    "test_dataset = test_dataset.prefetch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append(\"/home/azureuser/cloudfiles/code/Users/Mohammad.Shoaib/GSOC-22-Video-Swin-Transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python \"/home/azureuser/cloudfiles/code/Users/Mohammad.Shoaib/GSOC-22-Video-Swin-Transformers/convert.py\" -m \"swin_tiny_patch244_window877_kinetics400_1k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VideoSwinTransformer import model_configs, SwinTransformer3D, I3DHead_tf\n",
    "\n",
    "cfg_method = model_configs.MODEL_MAP[\"swin_tiny_patch244_window877_kinetics400_1k\"]\n",
    "cfg = cfg_method()\n",
    "\n",
    "name = cfg[\"name\"]\n",
    "link = cfg['link']\n",
    "del cfg[\"name\"]\n",
    "del cfg['link']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes,cfg,backbone, shape_of_input=(10,3,32,224,224)):\n",
    "    inputs = tf.keras.Input(shape_of_input[1:])\n",
    "    \n",
    "    x = backbone(inputs, training= True)\n",
    "    outputs = I3DHead_tf(num_classes, 768, training=True)(x)\n",
    "    return tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_of_input = (batch_size, 3, num_frames, resolution,resolution)\n",
    "\n",
    "num_epochs = 2\n",
    "sample_count = num_examples['train']\n",
    "warmup_epoch = 3\n",
    "total_steps = int(num_epochs * sample_count / batch_size)\n",
    "\n",
    "# Compute the number of warmup batches.\n",
    "warmup_steps = int(warmup_epoch * sample_count / batch_size)\n",
    "\n",
    "train_steps = num_examples['train'] // batch_size\n",
    "total_train_steps = train_steps * num_epochs\n",
    "test_steps = num_examples['test'] // batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_decay_with_warmup(global_step,\n",
    "                             learning_rate_base,\n",
    "                             total_steps,\n",
    "                             warmup_learning_rate=0.0,\n",
    "                             warmup_steps=0,\n",
    "                             hold_base_rate_steps=0):\n",
    "\n",
    "    if total_steps < warmup_steps:\n",
    "        raise ValueError('total_steps must be larger or equal to '\n",
    "                         'warmup_steps.')\n",
    "\n",
    "    if not isinstance(global_step, int):\n",
    "      global_step = 1\n",
    "     \n",
    "    learning_rate = 0.5 * learning_rate_base * (1 + np.cos(\n",
    "        np.pi *\n",
    "        (global_step - warmup_steps - hold_base_rate_steps\n",
    "        ) / float(total_steps - warmup_steps - hold_base_rate_steps)))\n",
    "\n",
    "\n",
    "    if hold_base_rate_steps > 0:\n",
    "        learning_rate = np.where(global_step > warmup_steps + hold_base_rate_steps,\n",
    "                                 learning_rate, learning_rate_base)\n",
    "    if warmup_steps > 0:\n",
    "        if learning_rate_base < warmup_learning_rate:\n",
    "            raise ValueError('learning_rate_base must be larger or equal to '\n",
    "                             'warmup_learning_rate.')\n",
    "        slope = (learning_rate_base - warmup_learning_rate) / warmup_steps\n",
    "        warmup_rate = slope * global_step + warmup_learning_rate\n",
    "        learning_rate = np.where(global_step < warmup_steps, warmup_rate,\n",
    "                                 learning_rate)\n",
    "        \n",
    "    return np.where(global_step > total_steps, 0.0, learning_rate)\n",
    "\n",
    "\n",
    "class CosineAnnealingSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, learning_rate_base,total_steps,warmup_learning_rate=0.0,warmup_steps=0,hold_base_rate_steps=0):\n",
    "    super().__init__()\n",
    "\n",
    "    \n",
    "    self.learning_rate_base = learning_rate_base\n",
    "    self.total_steps = total_steps\n",
    "    self.warmup_learning_rate = warmup_learning_rate\n",
    "    self.warmup_steps = warmup_steps\n",
    "    self.hold_base_rate_steps = hold_base_rate_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    lr = cosine_decay_with_warmup(global_step=step,\n",
    "                                      learning_rate_base=self.learning_rate_base,\n",
    "                                      total_steps=self.total_steps,\n",
    "                                      warmup_learning_rate=self.warmup_learning_rate,\n",
    "                                      warmup_steps=self.warmup_steps,\n",
    "                                      hold_base_rate_steps=self.hold_base_rate_steps)\n",
    "    print(\"lr =\", lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "\n",
    "    backbone = tf.keras.models.load_model(\"/home/azureuser/cloudfiles/code/Users/Datasets/swin_tiny_patch244_window877_kinetics400_1k_tf\")\n",
    "    # backbone = SwinTransformer3D(**cfg, shape_of_input=shape_of_input)\n",
    "    model = get_model(num_classes, cfg, backbone,shape_of_input=shape_of_input)\n",
    "\n",
    "    lr_backbone = CosineAnnealingSchedule(learning_rate_base=.001,\n",
    "                                        total_steps=total_steps,\n",
    "                                        warmup_learning_rate=0.0,\n",
    "                                        warmup_steps=warmup_steps,\n",
    "                                        hold_base_rate_steps=0)\n",
    "    lr_classifier = CosineAnnealingSchedule(learning_rate_base=.01,\n",
    "                                            total_steps=total_steps,\n",
    "                                            warmup_learning_rate=0.0,\n",
    "                                            warmup_steps=warmup_steps,\n",
    "                                            hold_base_rate_steps=0)\n",
    "\n",
    "    optimizer_backbone = tfa.optimizers.AdamW(weight_decay= 0.05,learning_rate=lr_backbone, beta_1= 0.9, beta_2=0.999, epsilon=1e-8)\n",
    "    optimizer_classifier = tfa.optimizers.AdamW(weight_decay= 0.05,learning_rate=lr_classifier, beta_1= 0.9, beta_2=0.999, epsilon=1e-8)\n",
    "\n",
    "\n",
    "\n",
    "    optimizers_and_layers = [(optimizer_backbone, model.layers[1]), (optimizer_classifier, model.layers[2])]\n",
    "    optimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers)\n",
    "\n",
    "    loss_obj = tf.keras.losses.CategoricalCrossentropy(from_logits=True,label_smoothing=0.1)\n",
    "    # loss_obj =  tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "    metrics=[\"top_k_categorical_accuracy\"] \n",
    "\n",
    "model.compile(loss=loss_obj, optimizer=optimizer, metrics=metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    }
   ],
   "source": [
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "# with strategy.scope():\n",
    "\n",
    "#     backbone = tf.keras.models.load_model(\"/home/azureuser/cloudfiles/code/Users/Datasets/swin_tiny_patch244_window877_kinetics400_1k_tf\")\n",
    "#     # backbone = SwinTransformer3D(**cfg, shape_of_input=shape_of_input)\n",
    "#     model = get_model(num_classes, cfg, backbone,shape_of_input=shape_of_input)\n",
    "\n",
    "#     optimizer = tfa.optimizers.AdamW(weight_decay= 0.05,learning_rate=3e-4, beta_1= 0.9, beta_2=0.999, epsilon=1e-8)\n",
    "#     metrics=[\"top_k_categorical_accuracy\"] \n",
    "#     loss_obj = tf.keras.losses.CategoricalCrossentropy(\n",
    "#     from_logits=True,\n",
    "#     label_smoothing=0.1,\n",
    "#     reduction=tf.keras.losses.Reduction.NONE)\n",
    "#     # loss_obj=tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# model.compile(loss=loss_obj, optimizer=optimizer, metrics=metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "print(os.getenv('TF_GPU_ALLOCATOR'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=num_epochs,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_steps=test_steps,\n",
    "    callbacks=[],\n",
    "    validation_freq=1,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_of_input = (1, 3, 32, 224,224)\n",
    "\n",
    "X = tf.random.normal(shape_of_input)\n",
    "y = tf.random.uniform((shape_of_input[0],1), 0, 5, tf.dtypes.int32)\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(5, cfg, shape_of_input=shape_of_input)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=loss_obj, optimizer=optimizer, metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.fit(\n",
    "    X,\n",
    "    y,\n",
    "    epochs=3,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_steps=test_steps,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ed124f2b279c186db57d0ede455df2b109954ad4fa1a5a2c59adb747c894aa2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
